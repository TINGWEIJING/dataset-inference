# MINGD
# --model_normalize 0, --data_normalize 0
# Teacher/Source/Victim model
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=0, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='mingd', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='teacher', model_dataset='CIFAR10', model_id='teacher', model_normalize=0, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_teacher_model-unnormalized_data-unnormalized
File Directory: ./files/CIFAR10/model_teacher_model-unnormalized_data-unnormalized
cuda:0
22-10-09 19:07
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Model: ./models/CIFAR10/model_teacher_model-unnormalized_data-unnormalized | 	 Test Acc: 0.939
Number of steps = 117 | Failed to convert = 0 | Time taken = 22.98202395439148
Number of steps = 140 | Failed to convert = 0 | Time taken = 24.386945486068726
Number of steps = 108 | Failed to convert = 0 | Time taken = 18.42782759666443
Number of steps = 142 | Failed to convert = 0 | Time taken = 19.36682915687561
Number of steps = 161 | Failed to convert = 0 | Time taken = 20.439094066619873
Number of steps = 240 | Failed to convert = 0 | Time taken = 24.218751668930054
Number of steps = 202 | Failed to convert = 0 | Time taken = 28.38068199157715
Number of steps = 227 | Failed to convert = 0 | Time taken = 26.301433324813843
Number of steps = 332 | Failed to convert = 0 | Time taken = 30.720027208328247
Number of steps = 149 | Failed to convert = 0 | Time taken = 24.227859020233154
Number of steps = 497 | Failed to convert = 0 | Time taken = 88.17067885398865
Number of steps = 500 | Failed to convert = 2 | Time taken = 97.62370324134827
Number of steps = 500 | Failed to convert = 1 | Time taken = 72.28329181671143
Number of steps = 497 | Failed to convert = 0 | Time taken = 73.41952514648438
Number of steps = 500 | Failed to convert = 3 | Time taken = 76.89293956756592
Number of steps = 500 | Failed to convert = 7 | Time taken = 89.24833226203918
Number of steps = 500 | Failed to convert = 15 | Time taken = 112.60181832313538
Number of steps = 500 | Failed to convert = 7 | Time taken = 103.48598337173462
Number of steps = 500 | Failed to convert = 6 | Time taken = 112.37056422233582
Number of steps = 500 | Failed to convert = 2 | Time taken = 97.08636522293091
Number of steps = 263 | Failed to convert = 0 | Time taken = 23.91055917739868
Number of steps = 145 | Failed to convert = 0 | Time taken = 23.019901752471924
Number of steps = 243 | Failed to convert = 0 | Time taken = 18.411386251449585
Number of steps = 153 | Failed to convert = 0 | Time taken = 17.61268925666809
Number of steps = 361 | Failed to convert = 0 | Time taken = 21.581323385238647
Number of steps = 309 | Failed to convert = 0 | Time taken = 25.520038843154907
Number of steps = 365 | Failed to convert = 0 | Time taken = 30.795929431915283
Number of steps = 324 | Failed to convert = 0 | Time taken = 30.52710723876953
Number of steps = 420 | Failed to convert = 0 | Time taken = 30.965346097946167
Number of steps = 183 | Failed to convert = 0 | Time taken = 23.874034881591797
Number of steps = 138 | Failed to convert = 0 | Time taken = 23.9656400680542
Number of steps = 129 | Failed to convert = 0 | Time taken = 23.71997833251953
Number of steps = 134 | Failed to convert = 0 | Time taken = 18.626899480819702
Number of steps = 97 | Failed to convert = 0 | Time taken = 18.312374114990234
Number of steps = 138 | Failed to convert = 0 | Time taken = 20.055750370025635
Number of steps = 132 | Failed to convert = 0 | Time taken = 21.347075939178467
Number of steps = 160 | Failed to convert = 0 | Time taken = 28.130568027496338
Number of steps = 163 | Failed to convert = 0 | Time taken = 24.945640563964844
Number of steps = 194 | Failed to convert = 0 | Time taken = 29.10431671142578
Number of steps = 143 | Failed to convert = 0 | Time taken = 23.058450937271118
Number of steps = 500 | Failed to convert = 4 | Time taken = 94.3924069404602
Number of steps = 500 | Failed to convert = 3 | Time taken = 93.36227750778198
Number of steps = 500 | Failed to convert = 3 | Time taken = 70.23582625389099
Number of steps = 500 | Failed to convert = 2 | Time taken = 71.6471700668335
Number of steps = 500 | Failed to convert = 4 | Time taken = 76.32240796089172
Number of steps = 500 | Failed to convert = 5 | Time taken = 83.75141453742981
Number of steps = 500 | Failed to convert = 15 | Time taken = 113.75177645683289
Number of steps = 500 | Failed to convert = 6 | Time taken = 98.01408457756042
Number of steps = 500 | Failed to convert = 12 | Time taken = 112.6347644329071
Number of steps = 500 | Failed to convert = 7 | Time taken = 89.9996600151062
Number of steps = 216 | Failed to convert = 0 | Time taken = 24.42303705215454
Number of steps = 137 | Failed to convert = 0 | Time taken = 22.067954063415527
Number of steps = 249 | Failed to convert = 0 | Time taken = 18.365516901016235
Number of steps = 189 | Failed to convert = 0 | Time taken = 17.993184328079224
Number of steps = 484 | Failed to convert = 0 | Time taken = 23.363205194473267
Number of steps = 365 | Failed to convert = 0 | Time taken = 25.040714740753174
Number of steps = 331 | Failed to convert = 0 | Time taken = 30.426403284072876
Number of steps = 383 | Failed to convert = 0 | Time taken = 29.39212942123413
Number of steps = 196 | Failed to convert = 0 | Time taken = 28.06353449821472
Number of steps = 154 | Failed to convert = 0 | Time taken = 22.124436140060425
torch.Size([1000, 10, 3])
tensor([[[1.8000e-02, 4.3413e-01, 1.4660e+01],
         [9.0000e-03, 2.9187e-01, 7.6000e+00],
         [1.0000e-02, 3.2268e-01, 7.4800e+00],
         ...,
         [1.1000e-02, 3.1040e-01, 7.8000e+00],
         [9.0000e-03, 2.9930e-01, 6.8000e+00],
         [2.9000e-02, 5.1004e-01, 1.6320e+01]],

        [[1.2000e-02, 3.2837e-01, 8.0375e+00],
         [6.0000e-03, 1.9050e-01, 3.9361e+00],
         [6.6000e-02, 8.1267e-01, 3.7829e+01],
         ...,
         [8.3000e-02, 7.5146e-01, 2.6005e+01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00],
         [1.9000e-02, 4.0221e-01, 1.0408e+01]],

        [[1.0000e-02, 1.9198e-01, 4.8600e+00],
         [2.0000e-03, 3.9797e-02, 1.0000e+00],
         [8.0000e-03, 1.8188e-01, 4.5800e+00],
         ...,
         [1.3000e-02, 2.5928e-01, 6.1600e+00],
         [0.0000e+00, 0.0000e+00, 0.0000e+00],
         [9.0000e-03, 1.8385e-01, 5.7400e+00]],

        ...,

        [[1.6000e-02, 3.6293e-01, 9.5622e+00],
         [0.0000e+00, 0.0000e+00, 0.0000e+00],
         [2.3000e-02, 4.6476e-01, 1.7434e+01],
         ...,
         [4.9000e-02, 7.7381e-01, 3.2013e+01],
         [9.0000e-03, 2.2623e-01, 6.9575e+00],
         [1.0000e-03, 2.9960e-02, 1.0000e+00]],

        [[3.2000e-02, 6.8697e-01, 3.5580e+01],
         [1.9000e-02, 5.7618e-01, 1.5580e+01],
         [1.2000e-02, 3.4525e-01, 7.8600e+00],
         ...,
         [2.2000e-02, 5.7116e-01, 1.7840e+01],
         [2.1000e-02, 6.8985e-01, 1.6140e+01],
         [4.7000e-02, 7.5456e-01, 2.2498e+01]],

        [[1.4000e-02, 3.2603e-01, 6.4000e+00],
         [9.0000e-03, 2.3848e-01, 4.3000e+00],
         [2.4000e-02, 5.1282e-01, 3.1140e+01],
         ...,
         [4.2000e-02, 6.7896e-01, 3.8980e+01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00],
         [1.5000e-02, 3.7398e-01, 8.8600e+00]]])
Number of steps = 137 | Failed to convert = 0 | Time taken = 26.535418033599854
Number of steps = 97 | Failed to convert = 0 | Time taken = 24.891435146331787
Number of steps = 101 | Failed to convert = 0 | Time taken = 18.392005681991577
Number of steps = 117 | Failed to convert = 0 | Time taken = 18.138458251953125
Number of steps = 102 | Failed to convert = 0 | Time taken = 19.030211210250854
Number of steps = 199 | Failed to convert = 0 | Time taken = 21.215805530548096
Number of steps = 178 | Failed to convert = 0 | Time taken = 27.301696300506592
Number of steps = 112 | Failed to convert = 0 | Time taken = 22.92874026298523
Number of steps = 296 | Failed to convert = 0 | Time taken = 33.528045654296875
Number of steps = 122 | Failed to convert = 0 | Time taken = 25.856865644454956
Number of steps = 500 | Failed to convert = 7 | Time taken = 104.67037534713745
Number of steps = 477 | Failed to convert = 0 | Time taken = 100.51908755302429
Number of steps = 500 | Failed to convert = 1 | Time taken = 71.07504391670227
Number of steps = 500 | Failed to convert = 1 | Time taken = 69.27075576782227
Number of steps = 500 | Failed to convert = 1 | Time taken = 74.46811962127686
Number of steps = 500 | Failed to convert = 1 | Time taken = 78.4856345653534
Number of steps = 500 | Failed to convert = 15 | Time taken = 107.18962907791138
Number of steps = 500 | Failed to convert = 1 | Time taken = 92.90020823478699
Number of steps = 500 | Failed to convert = 10 | Time taken = 122.63532543182373
Number of steps = 500 | Failed to convert = 6 | Time taken = 103.36843276023865
Number of steps = 194 | Failed to convert = 0 | Time taken = 25.406768560409546
Number of steps = 165 | Failed to convert = 0 | Time taken = 24.038682222366333
Number of steps = 172 | Failed to convert = 0 | Time taken = 16.63969588279724
Number of steps = 145 | Failed to convert = 0 | Time taken = 16.77063536643982
Number of steps = 215 | Failed to convert = 0 | Time taken = 18.526573419570923
Number of steps = 304 | Failed to convert = 0 | Time taken = 22.65457320213318
Number of steps = 354 | Failed to convert = 0 | Time taken = 28.709823846817017
Number of steps = 450 | Failed to convert = 0 | Time taken = 27.884477853775024
Number of steps = 256 | Failed to convert = 0 | Time taken = 30.256146669387817
Number of steps = 325 | Failed to convert = 0 | Time taken = 28.664111137390137
Number of steps = 136 | Failed to convert = 0 | Time taken = 24.17657971382141
Number of steps = 135 | Failed to convert = 0 | Time taken = 22.352794408798218
Number of steps = 127 | Failed to convert = 0 | Time taken = 20.465495347976685
Number of steps = 158 | Failed to convert = 0 | Time taken = 20.858449459075928
Number of steps = 105 | Failed to convert = 0 | Time taken = 21.47706699371338
Number of steps = 125 | Failed to convert = 0 | Time taken = 21.22317409515381
Number of steps = 207 | Failed to convert = 0 | Time taken = 30.189809560775757
Number of steps = 185 | Failed to convert = 0 | Time taken = 25.019010066986084
Number of steps = 327 | Failed to convert = 0 | Time taken = 32.59821176528931
Number of steps = 194 | Failed to convert = 0 | Time taken = 23.43094301223755
Number of steps = 500 | Failed to convert = 5 | Time taken = 91.67368793487549
Number of steps = 500 | Failed to convert = 4 | Time taken = 86.8067979812622
Number of steps = 500 | Failed to convert = 4 | Time taken = 79.98197746276855
Number of steps = 500 | Failed to convert = 2 | Time taken = 77.67682147026062
Number of steps = 500 | Failed to convert = 6 | Time taken = 86.8341109752655
Number of steps = 500 | Failed to convert = 5 | Time taken = 83.28184533119202
Number of steps = 500 | Failed to convert = 19 | Time taken = 122.38425040245056
Number of steps = 500 | Failed to convert = 7 | Time taken = 98.39168667793274
Number of steps = 500 | Failed to convert = 9 | Time taken = 119.32825946807861
Number of steps = 500 | Failed to convert = 7 | Time taken = 89.73062753677368
Number of steps = 287 | Failed to convert = 0 | Time taken = 23.485865831375122
Number of steps = 145 | Failed to convert = 0 | Time taken = 20.581136226654053
Number of steps = 270 | Failed to convert = 0 | Time taken = 21.649129152297974
Number of steps = 157 | Failed to convert = 0 | Time taken = 19.221087217330933
Number of steps = 395 | Failed to convert = 0 | Time taken = 25.460508346557617
Number of steps = 462 | Failed to convert = 0 | Time taken = 27.992675304412842
Number of steps = 340 | Failed to convert = 0 | Time taken = 34.289576053619385
Number of steps = 500 | Failed to convert = 1 | Time taken = 30.73811173439026
Number of steps = 213 | Failed to convert = 0 | Time taken = 30.689748764038086
Number of steps = 500 | Failed to convert = 1 | Time taken = 28.003015756607056
torch.Size([1000, 10, 3])
tensor([[[7.1000e-02, 5.9907e-01, 2.1085e+01],
         [1.7000e-02, 2.9497e-01, 7.2831e+00],
         [3.6000e-02, 3.7017e-01, 9.3692e+00],
         ...,
         [1.9000e-02, 3.1302e-01, 8.8812e+00],
         [2.4000e-02, 3.6904e-01, 1.1464e+01],
         [7.0000e-03, 2.0841e-01, 4.9878e+00]],

        [[2.1000e-02, 2.4717e-01, 1.1540e+01],
         [3.4000e-02, 3.2757e-01, 1.5180e+01],
         [4.0000e-03, 1.0156e-01, 2.9200e+00],
         ...,
         [9.0000e-03, 1.8127e-01, 7.1400e+00],
         [1.8000e-02, 2.3342e-01, 1.0260e+01],
         [2.4000e-02, 2.6627e-01, 1.3240e+01]],

        [[1.7000e-02, 3.8658e-01, 1.0560e+01],
         [2.6000e-02, 5.5948e-01, 1.4860e+01],
         [5.0000e-03, 1.6671e-01, 3.0000e+00],
         ...,
         [1.1000e-02, 2.7230e-01, 5.6600e+00],
         [7.7000e-02, 8.8696e-01, 3.5052e+01],
         [1.8000e-02, 4.0066e-01, 1.1280e+01]],

        ...,

        [[2.2000e-02, 7.3392e-01, 1.7474e+01],
         [2.2000e-02, 7.3324e-01, 1.6745e+01],
         [2.3000e-02, 7.6711e-01, 1.7579e+01],
         ...,
         [2.0000e-02, 6.8796e-01, 1.5724e+01],
         [2.7000e-02, 8.6967e-01, 2.2905e+01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00]],

        [[9.0000e-03, 2.6736e-01, 6.4520e+00],
         [1.3000e-02, 3.8828e-01, 9.9080e+00],
         [1.9000e-02, 3.4040e-01, 8.8229e+00],
         ...,
         [1.0200e-01, 5.9860e-01, 1.8628e+01],
         [3.1000e-02, 3.8323e-01, 1.1211e+01],
         [8.0000e-03, 2.1952e-01, 4.8390e+00]],

        [[0.0000e+00, 0.0000e+00, 0.0000e+00],
         [7.0000e-03, 2.2477e-01, 5.0000e+00],
         [6.0000e-03, 1.9434e-01, 4.0000e+00],
         ...,
         [8.0000e-03, 2.7280e-01, 6.0000e+00],
         [8.0000e-03, 2.6303e-01, 5.9753e+00],
         [6.0000e-03, 2.2360e-01, 4.9924e+00]]])
Time taken: 5672.58 s
# 'independent'
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=0, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='mingd', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='independent', model_dataset='CIFAR10', model_id='independent', model_normalize=0, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_independent_model-unnormalized_data-unnormalized
File Directory: ./files/CIFAR10/model_independent_model-unnormalized_data-unnormalized
cuda:0
22-10-09 20:41
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight". 
	size mismatch for module.block1.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 16, 3, 3]).
	size mismatch for module.block1.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([320, 160, 1, 1]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([640, 320, 1, 1]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.batch_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.fc.weight: copying a param with shape torch.Size([10, 64]) from checkpoint, the shape in current model is torch.Size([10, 640]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.batch_norm.num_batches_tracked", "module.fc.weight", "module.fc.bias". 
# C.2 'pre-act-18' | Different architecture
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=0, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='mingd', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='pre-act-18', model_dataset='CIFAR10', model_id='pre-act-18', model_normalize=0, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_pre-act-18_model-unnormalized_data-unnormalized
File Directory: ./files/CIFAR10/model_pre-act-18_model-unnormalized_data-unnormalized
cuda:0
22-10-09 20:41
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.fc.weight", "module.fc.bias". 
	Unexpected key(s) in state_dict: "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
	size mismatch for module.conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
# --model_normalize 0, --data_normalize 1
# Teacher/Source/Victim model
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=1, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='mingd', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='teacher', model_dataset='CIFAR10', model_id='teacher', model_normalize=0, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_teacher_model-unnormalized_data-normalized
File Directory: ./files/CIFAR10/model_teacher_model-unnormalized_data-normalized
cuda:0
22-10-09 20:41
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Model: ./models/CIFAR10/model_teacher_model-unnormalized_data-normalized | 	 Test Acc: 0.377
Number of steps = 144 | Failed to convert = 0 | Time taken = 16.81103277206421
Number of steps = 500 | Failed to convert = 13 | Time taken = 215.55303621292114
Number of steps = 455 | Failed to convert = 0 | Time taken = 36.59300112724304
Number of steps = 129 | Failed to convert = 0 | Time taken = 7.671177864074707
Number of steps = 300 | Failed to convert = 0 | Time taken = 29.1955144405365
Number of steps = 125 | Failed to convert = 0 | Time taken = 22.094082593917847
Number of steps = 217 | Failed to convert = 0 | Time taken = 13.76640272140503
Number of steps = 320 | Failed to convert = 0 | Time taken = 72.4384982585907
Number of steps = 113 | Failed to convert = 0 | Time taken = 16.576878786087036
Number of steps = 268 | Failed to convert = 0 | Time taken = 39.297041177749634
Number of steps = 476 | Failed to convert = 0 | Time taken = 53.26983380317688
Number of steps = 500 | Failed to convert = 303 | Time taken = 443.59212732315063
Number of steps = 500 | Failed to convert = 5 | Time taken = 102.12309050559998
Number of steps = 333 | Failed to convert = 0 | Time taken = 21.256622314453125
Number of steps = 500 | Failed to convert = 1 | Time taken = 84.5244836807251
Number of steps = 500 | Failed to convert = 3 | Time taken = 76.54758095741272
Number of steps = 500 | Failed to convert = 2 | Time taken = 40.045936822891235
Number of steps = 500 | Failed to convert = 66 | Time taken = 224.38016510009766
Number of steps = 500 | Failed to convert = 2 | Time taken = 55.81775903701782
Number of steps = 500 | Failed to convert = 8 | Time taken = 126.60342645645142
Number of steps = 93 | Failed to convert = 0 | Time taken = 10.15098261833191
Number of steps = 500 | Failed to convert = 61 | Time taken = 207.91637325286865
Number of steps = 380 | Failed to convert = 0 | Time taken = 22.458313703536987
Number of steps = 56 | Failed to convert = 0 | Time taken = 4.225328207015991
Number of steps = 336 | Failed to convert = 0 | Time taken = 18.1706280708313
Number of steps = 169 | Failed to convert = 0 | Time taken = 15.82204556465149
Number of steps = 117 | Failed to convert = 0 | Time taken = 8.169063806533813
Number of steps = 500 | Failed to convert = 4 | Time taken = 62.654284715652466
Number of steps = 379 | Failed to convert = 0 | Time taken = 14.531899690628052
Number of steps = 500 | Failed to convert = 1 | Time taken = 31.881750345230103
Number of steps = 154 | Failed to convert = 0 | Time taken = 17.12983989715576
Number of steps = 500 | Failed to convert = 12 | Time taken = 208.60505414009094
Number of steps = 304 | Failed to convert = 0 | Time taken = 35.00929570198059
Number of steps = 48 | Failed to convert = 0 | Time taken = 6.243217468261719
Number of steps = 203 | Failed to convert = 0 | Time taken = 27.074364185333252
Number of steps = 165 | Failed to convert = 0 | Time taken = 22.69300937652588
Number of steps = 79 | Failed to convert = 0 | Time taken = 11.679701566696167
Number of steps = 249 | Failed to convert = 0 | Time taken = 70.28276348114014
Number of steps = 150 | Failed to convert = 0 | Time taken = 17.16533088684082
Number of steps = 264 | Failed to convert = 0 | Time taken = 38.5605251789093
Number of steps = 500 | Failed to convert = 1 | Time taken = 54.97551608085632
Number of steps = 500 | Failed to convert = 300 | Time taken = 436.79743790626526
Number of steps = 500 | Failed to convert = 5 | Time taken = 102.03157615661621
Number of steps = 161 | Failed to convert = 0 | Time taken = 17.953185319900513
Number of steps = 500 | Failed to convert = 2 | Time taken = 86.2149555683136
Number of steps = 500 | Failed to convert = 5 | Time taken = 77.38212275505066
Number of steps = 297 | Failed to convert = 0 | Time taken = 36.83673405647278
Number of steps = 500 | Failed to convert = 71 | Time taken = 228.20330381393433
Number of steps = 500 | Failed to convert = 1 | Time taken = 56.67010188102722
Number of steps = 500 | Failed to convert = 9 | Time taken = 123.63422918319702
Number of steps = 166 | Failed to convert = 0 | Time taken = 11.429412603378296
Number of steps = 500 | Failed to convert = 39 | Time taken = 192.14394879341125
Number of steps = 440 | Failed to convert = 1 | Time taken = 23.20570993423462
Number of steps = 28 | Failed to convert = 0 | Time taken = 3.6427130699157715
Number of steps = 142 | Failed to convert = 0 | Time taken = 15.380929946899414
Number of steps = 354 | Failed to convert = 0 | Time taken = 19.322155237197876
Number of steps = 62 | Failed to convert = 0 | Time taken = 7.330425262451172
Number of steps = 500 | Failed to convert = 2 | Time taken = 62.723729372024536
Number of steps = 118 | Failed to convert = 0 | Time taken = 10.671689748764038
Number of steps = 500 | Failed to convert = 1 | Time taken = 29.828129529953003
torch.Size([1000, 10, 3])
tensor([[[2.1000e-02, 5.8923e-01, 1.3840e+01],
         [1.0500e-01, 1.0210e+00, 2.8030e+01],
         [1.3000e-02, 4.1848e-01, 8.0000e+00],
         ...,
         [4.6000e-02, 5.5910e-01, 1.6213e+01],
         [1.0000e-02, 2.9234e-01, 5.0000e+00],
         [3.4000e-02, 5.8462e-01, 1.4440e+01]],

        [[1.6000e-02, 4.8173e-01, 7.9157e+00],
         [9.9000e-02, 1.2042e+00, 3.3126e+01],
         [5.6000e-02, 1.1146e+00, 2.3387e+01],
         ...,
         [1.4200e-01, 1.2614e+00, 6.4971e+01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00],
         [3.9000e-02, 8.8683e-01, 1.7553e+01]],

        [[2.6000e-02, 6.6997e-01, 1.5700e+01],
         [1.6000e-01, 1.1563e+00, 5.1379e+01],
         [1.2000e-02, 3.1114e-01, 4.9775e+00],
         ...,
         [5.7000e-02, 8.7330e-01, 2.1360e+01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00],
         [1.3000e-02, 4.5331e-01, 9.0000e+00]],

        ...,

        [[1.2000e-02, 2.6169e-01, 5.8000e+00],
         [4.0000e-03, 1.1936e-01, 2.9000e+00],
         [2.0000e-02, 3.3295e-01, 1.0200e+01],
         ...,
         [8.0000e-03, 1.7871e-01, 4.3400e+00],
         [2.0000e-03, 6.9891e-02, 2.0000e+00],
         [0.0000e+00, 0.0000e+00, 0.0000e+00]],

        [[3.5000e-02, 7.8571e-01, 1.8240e+01],
         [1.9300e-01, 1.0780e+00, 1.0091e+02],
         [2.9000e-02, 6.9171e-01, 1.5720e+01],
         ...,
         [8.1000e-02, 1.0612e+00, 4.0399e+01],
         [2.2000e-02, 5.6203e-01, 1.1920e+01],
         [7.3000e-02, 9.6511e-01, 3.1257e+01]],

        [[9.0000e-03, 2.5528e-01, 4.0000e+00],
         [2.1000e-01, 1.0870e+00, 4.3036e+01],
         [5.8000e-02, 8.8082e-01, 1.9040e+01],
         ...,
         [1.0800e-01, 1.1360e+00, 2.9420e+01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00],
         [6.9000e-02, 1.0251e+00, 2.0960e+01]]])
Number of steps = 153 | Failed to convert = 0 | Time taken = 17.554434776306152
Number of steps = 500 | Failed to convert = 12 | Time taken = 213.4818012714386
Number of steps = 246 | Failed to convert = 0 | Time taken = 32.20249342918396
Number of steps = 97 | Failed to convert = 0 | Time taken = 7.113132476806641
Number of steps = 141 | Failed to convert = 0 | Time taken = 25.712459325790405
Number of steps = 198 | Failed to convert = 0 | Time taken = 24.01220941543579
Number of steps = 114 | Failed to convert = 0 | Time taken = 13.451649188995361
Number of steps = 500 | Failed to convert = 1 | Time taken = 76.04928088188171
Number of steps = 428 | Failed to convert = 0 | Time taken = 22.753095626831055
Number of steps = 454 | Failed to convert = 0 | Time taken = 43.185739517211914
Number of steps = 500 | Failed to convert = 1 | Time taken = 56.83632254600525
Number of steps = 500 | Failed to convert = 307 | Time taken = 446.423965215683
Number of steps = 500 | Failed to convert = 5 | Time taken = 96.46657204627991
Number of steps = 362 | Failed to convert = 0 | Time taken = 21.049546718597412
Number of steps = 500 | Failed to convert = 3 | Time taken = 82.70331454277039
Number of steps = 500 | Failed to convert = 7 | Time taken = 81.19292712211609
Number of steps = 425 | Failed to convert = 0 | Time taken = 42.42714715003967
Number of steps = 500 | Failed to convert = 69 | Time taken = 226.92745733261108
Number of steps = 500 | Failed to convert = 3 | Time taken = 62.89187932014465
Number of steps = 500 | Failed to convert = 13 | Time taken = 131.07213950157166
Number of steps = 147 | Failed to convert = 0 | Time taken = 11.674667596817017
Number of steps = 500 | Failed to convert = 43 | Time taken = 193.93331456184387
Number of steps = 230 | Failed to convert = 0 | Time taken = 18.575000524520874
Number of steps = 78 | Failed to convert = 0 | Time taken = 4.386369466781616
Number of steps = 247 | Failed to convert = 0 | Time taken = 16.71140742301941
Number of steps = 289 | Failed to convert = 0 | Time taken = 19.17913293838501
Number of steps = 166 | Failed to convert = 0 | Time taken = 9.505558729171753
Number of steps = 500 | Failed to convert = 8 | Time taken = 67.70994901657104
Number of steps = 189 | Failed to convert = 0 | Time taken = 13.004716873168945
Number of steps = 500 | Failed to convert = 5 | Time taken = 36.28001356124878
Number of steps = 114 | Failed to convert = 0 | Time taken = 16.370206117630005
Number of steps = 500 | Failed to convert = 20 | Time taken = 197.21927785873413
Number of steps = 165 | Failed to convert = 0 | Time taken = 30.982918977737427
Number of steps = 68 | Failed to convert = 0 | Time taken = 6.400928258895874
Number of steps = 134 | Failed to convert = 0 | Time taken = 24.65734577178955
Number of steps = 107 | Failed to convert = 0 | Time taken = 19.352028608322144
Number of steps = 124 | Failed to convert = 0 | Time taken = 13.11142897605896
Number of steps = 437 | Failed to convert = 0 | Time taken = 68.6782443523407
Number of steps = 180 | Failed to convert = 0 | Time taken = 18.20202088356018
Number of steps = 375 | Failed to convert = 0 | Time taken = 38.90037703514099
Number of steps = 348 | Failed to convert = 0 | Time taken = 51.888710021972656
Number of steps = 500 | Failed to convert = 285 | Time taken = 410.9315483570099
Number of steps = 500 | Failed to convert = 7 | Time taken = 95.91074085235596
Number of steps = 371 | Failed to convert = 0 | Time taken = 20.80829429626465
Number of steps = 500 | Failed to convert = 1 | Time taken = 78.68786358833313
Number of steps = 416 | Failed to convert = 0 | Time taken = 64.39967155456543
Number of steps = 500 | Failed to convert = 1 | Time taken = 42.96115469932556
Number of steps = 500 | Failed to convert = 57 | Time taken = 209.31048130989075
Number of steps = 500 | Failed to convert = 3 | Time taken = 59.04865312576294
Number of steps = 500 | Failed to convert = 14 | Time taken = 118.19706463813782
Number of steps = 156 | Failed to convert = 0 | Time taken = 11.267562866210938
Number of steps = 500 | Failed to convert = 47 | Time taken = 181.96676015853882
Number of steps = 195 | Failed to convert = 0 | Time taken = 18.783731698989868
Number of steps = 81 | Failed to convert = 0 | Time taken = 4.615488290786743
Number of steps = 500 | Failed to convert = 1 | Time taken = 20.872852563858032
Number of steps = 139 | Failed to convert = 0 | Time taken = 13.636671304702759
Number of steps = 149 | Failed to convert = 0 | Time taken = 9.194631576538086
Number of steps = 500 | Failed to convert = 4 | Time taken = 60.57001447677612
Number of steps = 222 | Failed to convert = 0 | Time taken = 13.043217897415161
Number of steps = 500 | Failed to convert = 5 | Time taken = 33.10497546195984
torch.Size([1000, 10, 3])
tensor([[[1.7000e-02, 3.9956e-01, 7.4600e+00],
         [6.2000e-02, 7.3853e-01, 1.4793e+01],
         [2.6000e-02, 5.2683e-01, 9.7400e+00],
         ...,
         [1.1600e-01, 1.0288e+00, 2.3010e+01],
         [1.3000e-02, 3.7040e-01, 6.9349e+00],
         [2.3000e-02, 4.6548e-01, 8.5120e+00]],

        [[1.2000e-02, 3.7213e-01, 7.8800e+00],
         [1.4600e-01, 1.3107e+00, 7.7743e+01],
         [7.0000e-03, 2.2597e-01, 5.0000e+00],
         ...,
         [1.1800e-01, 1.0070e+00, 1.9120e+01],
         [1.8000e-02, 4.5375e-01, 9.5000e+00],
         [5.2000e-02, 7.5946e-01, 2.1760e+01]],

        [[2.1000e-02, 5.4421e-01, 1.1280e+01],
         [8.7000e-02, 1.1468e+00, 4.7823e+01],
         [1.0000e-02, 3.1224e-01, 6.0000e+00],
         ...,
         [4.9000e-02, 7.2683e-01, 1.9700e+01],
         [4.8000e-02, 7.6045e-01, 2.2482e+01],
         [3.8000e-02, 7.3564e-01, 2.2589e+01]],

        ...,

        [[1.3000e-02, 3.4939e-01, 7.1800e+00],
         [1.4300e-01, 9.7464e-01, 2.5723e+01],
         [3.8000e-02, 5.7731e-01, 1.4515e+01],
         ...,
         [2.9000e-02, 5.4069e-01, 1.5029e+01],
         [7.0000e-03, 2.2753e-01, 3.9953e+00],
         [0.0000e+00, 0.0000e+00, 0.0000e+00]],

        [[1.4000e-02, 3.9748e-01, 6.9200e+00],
         [1.4600e-01, 1.2274e+00, 6.5733e+01],
         [2.6000e-02, 4.9902e-01, 7.8400e+00],
         ...,
         [8.0000e-02, 1.0908e+00, 8.4069e+01],
         [2.2000e-02, 6.2850e-01, 1.0686e+01],
         [3.4000e-02, 8.5794e-01, 2.2033e+01]],

        [[1.8000e-02, 3.8059e-01, 1.0335e+01],
         [4.2000e-02, 5.2572e-01, 1.5173e+01],
         [6.0000e-03, 1.8180e-01, 4.3800e+00],
         ...,
         [3.1000e-02, 4.5335e-01, 1.1848e+01],
         [5.0000e-03, 1.4241e-01, 3.7000e+00],
         [1.8000e-02, 3.7290e-01, 9.7796e+00]]])
Time taken: 8258.05 s
# 'independent'
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=1, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='mingd', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='independent', model_dataset='CIFAR10', model_id='independent', model_normalize=0, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_independent_model-unnormalized_data-normalized
File Directory: ./files/CIFAR10/model_independent_model-unnormalized_data-normalized
cuda:0
22-10-09 22:59
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight". 
	size mismatch for module.block1.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 16, 3, 3]).
	size mismatch for module.block1.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([320, 160, 1, 1]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([640, 320, 1, 1]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.batch_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.fc.weight: copying a param with shape torch.Size([10, 64]) from checkpoint, the shape in current model is torch.Size([10, 640]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.batch_norm.num_batches_tracked", "module.fc.weight", "module.fc.bias". 
# C.2 'pre-act-18' | Different architecture
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=1, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='mingd', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='pre-act-18', model_dataset='CIFAR10', model_id='pre-act-18', model_normalize=0, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_pre-act-18_model-unnormalized_data-normalized
File Directory: ./files/CIFAR10/model_pre-act-18_model-unnormalized_data-normalized
cuda:0
22-10-09 22:59
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.fc.weight", "module.fc.bias". 
	Unexpected key(s) in state_dict: "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
	size mismatch for module.conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
# --model_normalize 1, --data_normalize 0
# Teacher/Source/Victim model
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=0, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='mingd', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='teacher', model_dataset='CIFAR10', model_id='teacher', model_normalize=1, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_teacher_model-normalized_data-unnormalized
File Directory: ./files/CIFAR10/model_teacher_model-normalized_data-unnormalized
cuda:0
22-10-09 22:59
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Model: ./models/CIFAR10/model_teacher_model-normalized_data-unnormalized | 	 Test Acc: 0.942
Number of steps = 125 | Failed to convert = 0 | Time taken = 23.258225202560425
Number of steps = 121 | Failed to convert = 0 | Time taken = 27.030725955963135
Number of steps = 118 | Failed to convert = 0 | Time taken = 17.964113235473633
Number of steps = 121 | Failed to convert = 0 | Time taken = 17.11999249458313
Number of steps = 215 | Failed to convert = 0 | Time taken = 20.601378202438354
Number of steps = 142 | Failed to convert = 0 | Time taken = 21.77507448196411
Number of steps = 159 | Failed to convert = 0 | Time taken = 24.117441654205322
Number of steps = 205 | Failed to convert = 0 | Time taken = 23.535988330841064
Number of steps = 237 | Failed to convert = 0 | Time taken = 30.86756992340088
Number of steps = 116 | Failed to convert = 0 | Time taken = 23.259663343429565
Number of steps = 500 | Failed to convert = 3 | Time taken = 90.21837186813354
Number of steps = 500 | Failed to convert = 9 | Time taken = 112.45799541473389
Number of steps = 500 | Failed to convert = 1 | Time taken = 69.03295254707336
Number of steps = 500 | Failed to convert = 1 | Time taken = 64.42396569252014
Number of steps = 500 | Failed to convert = 2 | Time taken = 71.94629096984863
Number of steps = 500 | Failed to convert = 4 | Time taken = 88.80224704742432
Number of steps = 500 | Failed to convert = 6 | Time taken = 98.83029747009277
Number of steps = 500 | Failed to convert = 6 | Time taken = 89.82544088363647
Number of steps = 500 | Failed to convert = 10 | Time taken = 119.63871550559998
Number of steps = 500 | Failed to convert = 4 | Time taken = 97.63247776031494
Number of steps = 197 | Failed to convert = 0 | Time taken = 22.66970133781433
Number of steps = 189 | Failed to convert = 0 | Time taken = 28.17187237739563
Number of steps = 194 | Failed to convert = 0 | Time taken = 17.781057357788086
Number of steps = 142 | Failed to convert = 0 | Time taken = 15.042213678359985
Number of steps = 226 | Failed to convert = 0 | Time taken = 18.15237784385681
Number of steps = 319 | Failed to convert = 0 | Time taken = 26.771000146865845
Number of steps = 252 | Failed to convert = 0 | Time taken = 24.903082370758057
Number of steps = 195 | Failed to convert = 0 | Time taken = 23.550851821899414
Number of steps = 500 | Failed to convert = 1 | Time taken = 34.91900849342346
Number of steps = 224 | Failed to convert = 0 | Time taken = 23.96424627304077
Number of steps = 149 | Failed to convert = 0 | Time taken = 25.39165425300598
Number of steps = 149 | Failed to convert = 0 | Time taken = 27.181305408477783
Number of steps = 126 | Failed to convert = 0 | Time taken = 17.485003232955933
Number of steps = 72 | Failed to convert = 0 | Time taken = 15.921013355255127
Number of steps = 87 | Failed to convert = 0 | Time taken = 17.402048110961914
Number of steps = 111 | Failed to convert = 0 | Time taken = 20.05019998550415
Number of steps = 138 | Failed to convert = 0 | Time taken = 24.62380838394165
Number of steps = 138 | Failed to convert = 0 | Time taken = 22.530176877975464
Number of steps = 282 | Failed to convert = 0 | Time taken = 32.398133993148804
Number of steps = 146 | Failed to convert = 0 | Time taken = 24.25053644180298
Number of steps = 500 | Failed to convert = 5 | Time taken = 97.11499786376953
Number of steps = 500 | Failed to convert = 7 | Time taken = 110.19207787513733
Number of steps = 500 | Failed to convert = 2 | Time taken = 65.02698755264282
Number of steps = 428 | Failed to convert = 0 | Time taken = 61.7918176651001
Number of steps = 402 | Failed to convert = 0 | Time taken = 66.03721046447754
Number of steps = 468 | Failed to convert = 0 | Time taken = 78.14852237701416
Number of steps = 500 | Failed to convert = 8 | Time taken = 104.75923895835876
Number of steps = 500 | Failed to convert = 3 | Time taken = 87.98979473114014
Number of steps = 500 | Failed to convert = 19 | Time taken = 124.97891354560852
Number of steps = 500 | Failed to convert = 6 | Time taken = 96.44242548942566
Number of steps = 428 | Failed to convert = 0 | Time taken = 26.60959792137146
Number of steps = 252 | Failed to convert = 0 | Time taken = 27.371023178100586
Number of steps = 306 | Failed to convert = 0 | Time taken = 18.530606031417847
Number of steps = 164 | Failed to convert = 0 | Time taken = 15.22926115989685
Number of steps = 239 | Failed to convert = 0 | Time taken = 17.334819316864014
Number of steps = 245 | Failed to convert = 0 | Time taken = 23.06772208213806
Number of steps = 275 | Failed to convert = 0 | Time taken = 27.06977105140686
Number of steps = 440 | Failed to convert = 0 | Time taken = 26.704201698303223
Number of steps = 428 | Failed to convert = 0 | Time taken = 35.41335368156433
Number of steps = 188 | Failed to convert = 0 | Time taken = 23.301854848861694
torch.Size([1000, 10, 3])
tensor([[[1.0000e-02, 3.3559e-01, 6.9600e+00],
         [1.3000e-02, 3.5379e-01, 9.3200e+00],
         [1.1000e-02, 3.6221e-01, 7.9600e+00],
         ...,
         [1.6000e-02, 4.1494e-01, 1.1700e+01],
         [1.1000e-02, 3.4731e-01, 8.6600e+00],
         [5.1000e-02, 6.9768e-01, 2.8100e+01]],

        [[1.2000e-02, 3.1531e-01, 7.5722e+00],
         [5.0000e-03, 1.6223e-01, 2.9594e+00],
         [1.0200e-01, 7.8419e-01, 5.8578e+01],
         ...,
         [6.1000e-02, 7.5280e-01, 2.5703e+01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00],
         [2.3000e-02, 4.3191e-01, 1.3784e+01]],

        [[1.1000e-02, 2.0769e-01, 6.1200e+00],
         [2.0000e-03, 6.9477e-02, 2.0000e+00],
         [1.4000e-02, 2.5109e-01, 7.9200e+00],
         ...,
         [1.5000e-02, 2.6603e-01, 9.6400e+00],
         [0.0000e+00, 0.0000e+00, 0.0000e+00],
         [9.0000e-03, 1.7120e-01, 4.9400e+00]],

        ...,

        [[1.2000e-02, 2.5783e-01, 7.0543e+00],
         [2.0000e-03, 4.9840e-02, 1.0000e+00],
         [2.7000e-02, 4.6782e-01, 1.4487e+01],
         ...,
         [2.7000e-02, 4.4910e-01, 1.1947e+01],
         [8.0000e-03, 1.9358e-01, 5.2318e+00],
         [0.0000e+00, 0.0000e+00, 0.0000e+00]],

        [[4.5000e-02, 7.5630e-01, 2.3800e+01],
         [2.1000e-02, 5.8206e-01, 1.6820e+01],
         [1.0000e-02, 3.2106e-01, 6.8800e+00],
         ...,
         [2.7000e-02, 6.0478e-01, 1.9180e+01],
         [1.4000e-02, 4.3789e-01, 8.8000e+00],
         [4.4000e-02, 6.9994e-01, 2.0740e+01]],

        [[1.4000e-02, 4.0284e-01, 1.1480e+01],
         [1.0000e-02, 3.1495e-01, 7.3400e+00],
         [1.6000e-02, 4.9299e-01, 1.0900e+01],
         ...,
         [4.5000e-02, 6.6043e-01, 4.3300e+01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00],
         [1.3000e-02, 4.2569e-01, 9.5000e+00]]])
Number of steps = 151 | Failed to convert = 0 | Time taken = 27.761433124542236
Number of steps = 128 | Failed to convert = 0 | Time taken = 28.928881645202637
Number of steps = 100 | Failed to convert = 0 | Time taken = 17.623570442199707
Number of steps = 86 | Failed to convert = 0 | Time taken = 16.021783351898193
Number of steps = 93 | Failed to convert = 0 | Time taken = 17.720739603042603
Number of steps = 115 | Failed to convert = 0 | Time taken = 19.11463737487793
Number of steps = 167 | Failed to convert = 0 | Time taken = 24.466779232025146
Number of steps = 132 | Failed to convert = 0 | Time taken = 21.206628799438477
Number of steps = 240 | Failed to convert = 0 | Time taken = 35.08241510391235
Number of steps = 164 | Failed to convert = 0 | Time taken = 26.997438192367554
Number of steps = 500 | Failed to convert = 8 | Time taken = 108.42065906524658
Number of steps = 500 | Failed to convert = 4 | Time taken = 119.18227767944336
Number of steps = 500 | Failed to convert = 1 | Time taken = 68.1345386505127
Number of steps = 373 | Failed to convert = 0 | Time taken = 60.213685035705566
Number of steps = 500 | Failed to convert = 1 | Time taken = 68.4090645313263
Number of steps = 500 | Failed to convert = 1 | Time taken = 75.36640548706055
Number of steps = 500 | Failed to convert = 9 | Time taken = 96.57727265357971
Number of steps = 500 | Failed to convert = 2 | Time taken = 83.11482977867126
Number of steps = 500 | Failed to convert = 15 | Time taken = 131.33385133743286
Number of steps = 500 | Failed to convert = 5 | Time taken = 105.55149674415588
Number of steps = 309 | Failed to convert = 0 | Time taken = 28.82817006111145
Number of steps = 179 | Failed to convert = 0 | Time taken = 27.908965349197388
Number of steps = 148 | Failed to convert = 0 | Time taken = 16.28314447402954
Number of steps = 132 | Failed to convert = 0 | Time taken = 14.246429681777954
Number of steps = 140 | Failed to convert = 0 | Time taken = 15.516489505767822
Number of steps = 296 | Failed to convert = 0 | Time taken = 21.72700262069702
Number of steps = 329 | Failed to convert = 0 | Time taken = 25.260823011398315
Number of steps = 189 | Failed to convert = 0 | Time taken = 21.02843427658081
Number of steps = 500 | Failed to convert = 1 | Time taken = 39.152557134628296
Number of steps = 184 | Failed to convert = 0 | Time taken = 26.55081868171692
Number of steps = 152 | Failed to convert = 0 | Time taken = 23.575572729110718
Number of steps = 164 | Failed to convert = 0 | Time taken = 24.473786115646362
Number of steps = 155 | Failed to convert = 0 | Time taken = 20.087772130966187
Number of steps = 122 | Failed to convert = 0 | Time taken = 18.418945789337158
Number of steps = 93 | Failed to convert = 0 | Time taken = 20.29927349090576
Number of steps = 128 | Failed to convert = 0 | Time taken = 20.382217407226562
Number of steps = 231 | Failed to convert = 0 | Time taken = 26.905882120132446
Number of steps = 111 | Failed to convert = 0 | Time taken = 21.382994651794434
Number of steps = 311 | Failed to convert = 0 | Time taken = 33.09736108779907
Number of steps = 173 | Failed to convert = 0 | Time taken = 23.411494493484497
Number of steps = 500 | Failed to convert = 2 | Time taken = 89.0137619972229
Number of steps = 500 | Failed to convert = 3 | Time taken = 96.34229636192322
Number of steps = 500 | Failed to convert = 3 | Time taken = 77.1000349521637
Number of steps = 500 | Failed to convert = 1 | Time taken = 69.30746936798096
Number of steps = 500 | Failed to convert = 1 | Time taken = 80.56176590919495
Number of steps = 500 | Failed to convert = 3 | Time taken = 81.69531846046448
Number of steps = 500 | Failed to convert = 11 | Time taken = 107.7927987575531
Number of steps = 500 | Failed to convert = 1 | Time taken = 86.22660064697266
Number of steps = 500 | Failed to convert = 10 | Time taken = 121.3163833618164
Number of steps = 500 | Failed to convert = 4 | Time taken = 90.59033179283142
Number of steps = 259 | Failed to convert = 0 | Time taken = 23.33652949333191
Number of steps = 309 | Failed to convert = 0 | Time taken = 26.072492361068726
Number of steps = 344 | Failed to convert = 0 | Time taken = 22.452949047088623
Number of steps = 280 | Failed to convert = 0 | Time taken = 19.319526433944702
Number of steps = 334 | Failed to convert = 0 | Time taken = 21.763195991516113
Number of steps = 500 | Failed to convert = 1 | Time taken = 28.68304753303528
Number of steps = 464 | Failed to convert = 0 | Time taken = 32.27061867713928
Number of steps = 368 | Failed to convert = 0 | Time taken = 25.354464054107666
Number of steps = 500 | Failed to convert = 1 | Time taken = 36.4006233215332
Number of steps = 226 | Failed to convert = 0 | Time taken = 23.400179862976074
torch.Size([1000, 10, 3])
tensor([[[3.9000e-02, 4.7104e-01, 1.8573e+01],
         [1.9000e-02, 3.1820e-01, 1.0082e+01],
         [1.4000e-02, 2.5270e-01, 6.3414e+00],
         ...,
         [1.4000e-02, 2.5162e-01, 6.9745e+00],
         [1.3000e-02, 2.5789e-01, 8.1749e+00],
         [8.0000e-03, 2.0481e-01, 5.3939e+00]],

        [[3.4000e-02, 4.0082e-01, 1.8160e+01],
         [3.4000e-02, 4.6525e-01, 1.7940e+01],
         [5.0000e-03, 1.1066e-01, 3.8000e+00],
         ...,
         [7.0000e-03, 1.6286e-01, 5.0800e+00],
         [1.8000e-02, 3.1659e-01, 1.3540e+01],
         [1.1000e-02, 2.0467e-01, 9.1200e+00]],

        [[2.5000e-02, 4.9266e-01, 1.4980e+01],
         [4.7000e-02, 6.8930e-01, 2.0520e+01],
         [9.0000e-03, 3.0282e-01, 6.9000e+00],
         ...,
         [1.2000e-02, 2.7115e-01, 5.6000e+00],
         [1.3400e-01, 1.0928e+00, 5.0909e+01],
         [1.8000e-02, 4.1059e-01, 1.0880e+01]],

        ...,

        [[2.0000e-02, 6.0371e-01, 1.3458e+01],
         [2.1000e-02, 6.6131e-01, 1.5269e+01],
         [1.9000e-02, 6.1680e-01, 1.3579e+01],
         ...,
         [1.8000e-02, 6.5100e-01, 1.3878e+01],
         [1.8000e-02, 5.9257e-01, 1.3600e+01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00]],

        [[8.0000e-03, 2.2891e-01, 4.6790e+00],
         [9.0000e-03, 2.5987e-01, 5.8512e+00],
         [1.2000e-02, 2.4907e-01, 4.8522e+00],
         ...,
         [8.3000e-02, 3.7448e-01, 1.3911e+01],
         [1.5000e-02, 3.1048e-01, 8.1663e+00],
         [7.0000e-03, 2.0020e-01, 3.9747e+00]],

        [[0.0000e+00, 0.0000e+00, 0.0000e+00],
         [7.0000e-03, 2.4413e-01, 4.9945e+00],
         [6.0000e-03, 2.0147e-01, 4.9331e+00],
         ...,
         [9.0000e-03, 3.1759e-01, 7.0000e+00],
         [7.0000e-03, 2.5083e-01, 4.9953e+00],
         [8.0000e-03, 2.9016e-01, 5.9953e+00]]])
Time taken: 5552.16 s
# 'independent'
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=0, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='mingd', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='independent', model_dataset='CIFAR10', model_id='independent', model_normalize=1, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_independent_model-normalized_data-unnormalized
File Directory: ./files/CIFAR10/model_independent_model-normalized_data-unnormalized
cuda:0
22-10-10 00:32
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight". 
	size mismatch for module.block1.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 16, 3, 3]).
	size mismatch for module.block1.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([320, 160, 1, 1]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([640, 320, 1, 1]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.batch_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.fc.weight: copying a param with shape torch.Size([10, 64]) from checkpoint, the shape in current model is torch.Size([10, 640]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.batch_norm.num_batches_tracked", "module.fc.weight", "module.fc.bias". 
# C.2 'pre-act-18' | Different architecture
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=0, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='mingd', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='pre-act-18', model_dataset='CIFAR10', model_id='pre-act-18', model_normalize=1, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_pre-act-18_model-normalized_data-unnormalized
File Directory: ./files/CIFAR10/model_pre-act-18_model-normalized_data-unnormalized
cuda:0
22-10-10 00:32
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.fc.weight", "module.fc.bias". 
	Unexpected key(s) in state_dict: "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
	size mismatch for module.conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
# --model_normalize 1, --data_normalize 1
# Teacher/Source/Victim model
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=1, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='mingd', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='teacher', model_dataset='CIFAR10', model_id='teacher', model_normalize=1, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_teacher_model-normalized_data-normalized
File Directory: ./files/CIFAR10/model_teacher_model-normalized_data-normalized
cuda:0
22-10-10 00:32
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Model: ./models/CIFAR10/model_teacher_model-normalized_data-normalized | 	 Test Acc: 0.398
Number of steps = 176 | Failed to convert = 0 | Time taken = 23.8128445148468
Number of steps = 500 | Failed to convert = 1 | Time taken = 129.33513116836548
Number of steps = 397 | Failed to convert = 0 | Time taken = 38.01362586021423
Number of steps = 74 | Failed to convert = 0 | Time taken = 6.438628911972046
Number of steps = 216 | Failed to convert = 0 | Time taken = 23.647932529449463
Number of steps = 417 | Failed to convert = 0 | Time taken = 28.164870023727417
Number of steps = 150 | Failed to convert = 0 | Time taken = 16.662519931793213
Number of steps = 406 | Failed to convert = 0 | Time taken = 49.15350651741028
Number of steps = 104 | Failed to convert = 0 | Time taken = 17.051679134368896
Number of steps = 448 | Failed to convert = 0 | Time taken = 71.59353041648865
Number of steps = 500 | Failed to convert = 1 | Time taken = 75.7376172542572
Number of steps = 500 | Failed to convert = 189 | Time taken = 363.4604253768921
Number of steps = 500 | Failed to convert = 3 | Time taken = 107.3197090625763
Number of steps = 439 | Failed to convert = 0 | Time taken = 21.869366884231567
Number of steps = 500 | Failed to convert = 2 | Time taken = 71.04860639572144
Number of steps = 500 | Failed to convert = 12 | Time taken = 81.11933374404907
Number of steps = 500 | Failed to convert = 2 | Time taken = 54.684534788131714
Number of steps = 500 | Failed to convert = 15 | Time taken = 153.71444129943848
Number of steps = 500 | Failed to convert = 1 | Time taken = 58.44564867019653
Number of steps = 500 | Failed to convert = 62 | Time taken = 212.16093802452087
Number of steps = 95 | Failed to convert = 0 | Time taken = 13.758729457855225
Number of steps = 500 | Failed to convert = 11 | Time taken = 115.91060304641724
Number of steps = 304 | Failed to convert = 0 | Time taken = 21.96109914779663
Number of steps = 53 | Failed to convert = 0 | Time taken = 4.038843870162964
Number of steps = 127 | Failed to convert = 0 | Time taken = 13.094975471496582
Number of steps = 500 | Failed to convert = 1 | Time taken = 23.654019832611084
Number of steps = 274 | Failed to convert = 0 | Time taken = 12.953022480010986
Number of steps = 500 | Failed to convert = 3 | Time taken = 39.372538566589355
Number of steps = 456 | Failed to convert = 0 | Time taken = 16.9733726978302
Number of steps = 500 | Failed to convert = 4 | Time taken = 59.880590200424194
Number of steps = 303 | Failed to convert = 0 | Time taken = 26.40103006362915
Number of steps = 441 | Failed to convert = 0 | Time taken = 133.89330577850342
Number of steps = 199 | Failed to convert = 0 | Time taken = 34.41274428367615
Number of steps = 65 | Failed to convert = 0 | Time taken = 5.846914052963257
Number of steps = 190 | Failed to convert = 0 | Time taken = 23.735568523406982
Number of steps = 130 | Failed to convert = 0 | Time taken = 22.477165699005127
Number of steps = 118 | Failed to convert = 0 | Time taken = 16.659557580947876
Number of steps = 263 | Failed to convert = 0 | Time taken = 47.5772123336792
Number of steps = 126 | Failed to convert = 0 | Time taken = 16.95129108428955
Number of steps = 384 | Failed to convert = 0 | Time taken = 68.58828616142273
Number of steps = 500 | Failed to convert = 3 | Time taken = 77.1903567314148
Number of steps = 500 | Failed to convert = 199 | Time taken = 371.67010259628296
Number of steps = 500 | Failed to convert = 9 | Time taken = 106.95050740242004
Number of steps = 280 | Failed to convert = 0 | Time taken = 18.055478811264038
Number of steps = 500 | Failed to convert = 4 | Time taken = 73.1981201171875
Number of steps = 500 | Failed to convert = 5 | Time taken = 79.4612307548523
Number of steps = 487 | Failed to convert = 0 | Time taken = 54.83723306655884
Number of steps = 500 | Failed to convert = 27 | Time taken = 155.48613047599792
Number of steps = 500 | Failed to convert = 1 | Time taken = 57.85323929786682
Number of steps = 500 | Failed to convert = 52 | Time taken = 208.40071535110474
Number of steps = 112 | Failed to convert = 0 | Time taken = 14.352779150009155
Number of steps = 500 | Failed to convert = 9 | Time taken = 121.78375506401062
Number of steps = 293 | Failed to convert = 0 | Time taken = 21.608335494995117
Number of steps = 56 | Failed to convert = 0 | Time taken = 3.945033550262451
Number of steps = 140 | Failed to convert = 0 | Time taken = 13.545958042144775
Number of steps = 466 | Failed to convert = 0 | Time taken = 21.75225520133972
Number of steps = 153 | Failed to convert = 0 | Time taken = 11.605218410491943
Number of steps = 500 | Failed to convert = 4 | Time taken = 44.4156494140625
Number of steps = 338 | Failed to convert = 0 | Time taken = 14.682584047317505
Number of steps = 500 | Failed to convert = 1 | Time taken = 56.08543872833252
torch.Size([1000, 10, 3])
tensor([[[2.1000e-02, 4.7490e-01, 1.2360e+01],
         [1.0400e-01, 8.8522e-01, 2.7846e+01],
         [1.4000e-02, 3.8395e-01, 8.0400e+00],
         ...,
         [9.7000e-02, 1.0075e+00, 3.9315e+01],
         [7.0000e-03, 2.0769e-01, 4.0000e+00],
         [5.6000e-02, 6.5900e-01, 2.3814e+01]],

        [[1.1000e-02, 3.3792e-01, 5.9745e+00],
         [2.8000e-02, 7.1328e-01, 1.5490e+01],
         [6.4000e-02, 9.8945e-01, 2.2100e+01],
         ...,
         [6.0000e-02, 1.0212e+00, 2.8767e+01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00],
         [2.9000e-02, 7.2151e-01, 1.6321e+01]],

        [[5.4000e-02, 7.3007e-01, 1.7600e+01],
         [1.0000e-01, 9.7413e-01, 5.7535e+01],
         [9.0000e-03, 2.4570e-01, 4.0000e+00],
         ...,
         [2.5000e-02, 4.0572e-01, 7.3220e+00],
         [0.0000e+00, 0.0000e+00, 0.0000e+00],
         [5.6000e-02, 7.2329e-01, 1.7807e+01]],

        ...,

        [[1.2000e-02, 2.3773e-01, 5.9178e+00],
         [2.0000e-03, 7.2627e-02, 2.0000e+00],
         [9.0000e-03, 2.0322e-01, 4.7400e+00],
         ...,
         [2.0000e-03, 6.3864e-02, 1.8600e+00],
         [2.0000e-03, 5.9931e-02, 1.9400e+00],
         [0.0000e+00, 0.0000e+00, 0.0000e+00]],

        [[4.2000e-02, 8.0849e-01, 1.7760e+01],
         [1.3100e-01, 1.1343e+00, 3.0915e+01],
         [2.5000e-02, 6.9667e-01, 1.5860e+01],
         ...,
         [5.0000e-02, 8.6204e-01, 2.5100e+01],
         [2.0000e-02, 5.8657e-01, 1.0720e+01],
         [8.1000e-02, 8.8443e-01, 2.2111e+01]],

        [[1.1000e-02, 3.1285e-01, 5.0000e+00],
         [1.0300e-01, 1.0728e+00, 2.6180e+01],
         [5.3000e-02, 7.7023e-01, 1.6400e+01],
         ...,
         [5.7000e-02, 8.3778e-01, 1.8540e+01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00],
         [6.6000e-02, 7.6927e-01, 1.7480e+01]]])
Number of steps = 168 | Failed to convert = 1 | Time taken = 25.22080111503601
Number of steps = 500 | Failed to convert = 1 | Time taken = 140.23474717140198
Number of steps = 160 | Failed to convert = 0 | Time taken = 32.88964056968689
Number of steps = 122 | Failed to convert = 0 | Time taken = 6.576624870300293
Number of steps = 207 | Failed to convert = 0 | Time taken = 24.340530157089233
Number of steps = 154 | Failed to convert = 0 | Time taken = 22.83363151550293
Number of steps = 204 | Failed to convert = 0 | Time taken = 19.399089574813843
Number of steps = 254 | Failed to convert = 0 | Time taken = 50.080496311187744
Number of steps = 99 | Failed to convert = 0 | Time taken = 18.070821046829224
Number of steps = 456 | Failed to convert = 0 | Time taken = 74.06213617324829
Number of steps = 500 | Failed to convert = 4 | Time taken = 80.3915491104126
Number of steps = 500 | Failed to convert = 212 | Time taken = 375.0441572666168
Number of steps = 500 | Failed to convert = 5 | Time taken = 104.33766078948975
Number of steps = 500 | Failed to convert = 1 | Time taken = 21.478169918060303
Number of steps = 500 | Failed to convert = 3 | Time taken = 74.05308556556702
Number of steps = 500 | Failed to convert = 9 | Time taken = 81.21593809127808
Number of steps = 500 | Failed to convert = 7 | Time taken = 61.566423416137695
Number of steps = 500 | Failed to convert = 25 | Time taken = 166.9943141937256
Number of steps = 500 | Failed to convert = 1 | Time taken = 63.39170789718628
Number of steps = 500 | Failed to convert = 57 | Time taken = 209.3577401638031
Number of steps = 158 | Failed to convert = 0 | Time taken = 15.943651914596558
Number of steps = 500 | Failed to convert = 9 | Time taken = 124.60284066200256
Number of steps = 333 | Failed to convert = 0 | Time taken = 21.84501004219055
Number of steps = 86 | Failed to convert = 0 | Time taken = 4.610499858856201
Number of steps = 291 | Failed to convert = 0 | Time taken = 15.919110536575317
Number of steps = 411 | Failed to convert = 0 | Time taken = 22.30905556678772
Number of steps = 202 | Failed to convert = 0 | Time taken = 13.641528844833374
Number of steps = 500 | Failed to convert = 1 | Time taken = 41.17704129219055
Number of steps = 113 | Failed to convert = 0 | Time taken = 11.978568077087402
Number of steps = 500 | Failed to convert = 5 | Time taken = 59.49029994010925
Number of steps = 132 | Failed to convert = 0 | Time taken = 22.101195812225342
Number of steps = 500 | Failed to convert = 3 | Time taken = 127.60738706588745
Number of steps = 209 | Failed to convert = 0 | Time taken = 32.22349405288696
Number of steps = 63 | Failed to convert = 0 | Time taken = 5.655492782592773
Number of steps = 178 | Failed to convert = 0 | Time taken = 23.364538192749023
Number of steps = 181 | Failed to convert = 0 | Time taken = 22.112982988357544
Number of steps = 122 | Failed to convert = 0 | Time taken = 16.67087173461914
Number of steps = 241 | Failed to convert = 0 | Time taken = 45.85246658325195
Number of steps = 179 | Failed to convert = 0 | Time taken = 18.125837087631226
Number of steps = 500 | Failed to convert = 1 | Time taken = 68.78223538398743
Number of steps = 500 | Failed to convert = 1 | Time taken = 72.91849780082703
Number of steps = 500 | Failed to convert = 180 | Time taken = 347.38944268226624
Number of steps = 500 | Failed to convert = 4 | Time taken = 100.07589054107666
Number of steps = 296 | Failed to convert = 0 | Time taken = 18.472174882888794
Number of steps = 500 | Failed to convert = 2 | Time taken = 72.6365373134613
Number of steps = 500 | Failed to convert = 5 | Time taken = 74.61445355415344
Number of steps = 500 | Failed to convert = 3 | Time taken = 55.743025064468384
Number of steps = 500 | Failed to convert = 17 | Time taken = 151.15931677818298
Number of steps = 500 | Failed to convert = 4 | Time taken = 58.875606298446655
Number of steps = 500 | Failed to convert = 50 | Time taken = 195.59748125076294
Number of steps = 102 | Failed to convert = 0 | Time taken = 13.61121916770935
Number of steps = 500 | Failed to convert = 9 | Time taken = 110.5968496799469
Number of steps = 97 | Failed to convert = 0 | Time taken = 17.177748441696167
Number of steps = 56 | Failed to convert = 0 | Time taken = 3.871983766555786
Number of steps = 167 | Failed to convert = 0 | Time taken = 13.592419624328613
Number of steps = 500 | Failed to convert = 1 | Time taken = 20.689913988113403
Number of steps = 187 | Failed to convert = 0 | Time taken = 11.81649112701416
Number of steps = 500 | Failed to convert = 1 | Time taken = 39.74576377868652
Number of steps = 151 | Failed to convert = 0 | Time taken = 11.561568975448608
Number of steps = 500 | Failed to convert = 3 | Time taken = 54.39370846748352
torch.Size([1000, 10, 3])
tensor([[[1.3000e-02, 3.7484e-01, 6.9824e+00],
         [4.0000e-02, 7.3364e-01, 1.4506e+01],
         [2.1000e-02, 5.0624e-01, 8.4145e+00],
         ...,
         [1.1500e-01, 9.7742e-01, 2.7652e+01],
         [1.1000e-02, 3.0867e-01, 4.9345e+00],
         [1.5000e-02, 4.1161e-01, 6.9084e+00]],

        [[2.5000e-02, 5.3077e-01, 1.2820e+01],
         [1.9800e-01, 9.4153e-01, 8.4854e+01],
         [7.0000e-03, 2.3516e-01, 4.0000e+00],
         ...,
         [6.4000e-02, 6.7203e-01, 2.3760e+01],
         [1.6000e-02, 4.5760e-01, 1.1460e+01],
         [9.0000e-02, 8.8542e-01, 2.9280e+01]],

        [[2.0000e-02, 5.8047e-01, 1.2380e+01],
         [1.1600e-01, 1.1838e+00, 5.0499e+01],
         [1.1000e-02, 3.4615e-01, 7.0000e+00],
         ...,
         [3.7000e-02, 6.7927e-01, 1.9500e+01],
         [3.7000e-02, 7.2291e-01, 1.8880e+01],
         [7.3000e-02, 1.0569e+00, 4.4627e+01]],

        ...,

        [[1.9000e-02, 5.0708e-01, 1.1256e+01],
         [1.4400e-01, 1.0653e+00, 2.7988e+01],
         [3.3000e-02, 7.2113e-01, 1.7547e+01],
         ...,
         [1.8000e-02, 4.5329e-01, 9.2280e+00],
         [1.7000e-02, 5.0616e-01, 1.1664e+01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00]],

        [[1.6000e-02, 4.0229e-01, 5.9498e+00],
         [8.0000e-02, 1.0519e+00, 2.7607e+01],
         [1.5000e-02, 3.6476e-01, 5.9280e+00],
         ...,
         [3.6000e-02, 7.8722e-01, 1.5166e+01],
         [1.8000e-02, 4.7475e-01, 7.8976e+00],
         [4.0000e-02, 7.1269e-01, 1.3310e+01]],

        [[1.9000e-02, 3.7249e-01, 1.0058e+01],
         [4.4000e-02, 6.0289e-01, 1.7669e+01],
         [1.7000e-02, 3.3012e-01, 9.0400e+00],
         ...,
         [1.8000e-02, 3.4708e-01, 1.0680e+01],
         [3.0000e-03, 1.0955e-01, 2.0000e+00],
         [7.8000e-02, 7.4822e-01, 3.4499e+01]]])
Time taken: 7703.71 s
# 'independent'
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=1, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='mingd', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='independent', model_dataset='CIFAR10', model_id='independent', model_normalize=1, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_independent_model-normalized_data-normalized
File Directory: ./files/CIFAR10/model_independent_model-normalized_data-normalized
cuda:0
22-10-10 02:41
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight". 
	size mismatch for module.block1.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 16, 3, 3]).
	size mismatch for module.block1.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([320, 160, 1, 1]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([640, 320, 1, 1]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.batch_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.fc.weight: copying a param with shape torch.Size([10, 64]) from checkpoint, the shape in current model is torch.Size([10, 640]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.batch_norm.num_batches_tracked", "module.fc.weight", "module.fc.bias". 
# C.2 'pre-act-18' | Different architecture
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=1, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='mingd', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='pre-act-18', model_dataset='CIFAR10', model_id='pre-act-18', model_normalize=1, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_pre-act-18_model-normalized_data-normalized
File Directory: ./files/CIFAR10/model_pre-act-18_model-normalized_data-normalized
cuda:0
22-10-10 02:41
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.fc.weight", "module.fc.bias". 
	Unexpected key(s) in state_dict: "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
	size mismatch for module.conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
# RAND
# --model_normalize 0, --data_normalize 0
# Teacher/Source/Victim model
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=0, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='rand', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='teacher', model_dataset='CIFAR10', model_id='teacher', model_normalize=0, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_teacher_model-unnormalized_data-unnormalized
File Directory: ./files/CIFAR10/model_teacher_model-unnormalized_data-unnormalized
cuda:0
22-10-10 02:41
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Model: ./models/CIFAR10/model_teacher_model-unnormalized_data-unnormalized | 	 Test Acc: 0.939
Getting random attacks
Number of steps = 50 | Failed to convert = 60 | Time taken = 12.74748158454895
Number of steps = 50 | Failed to convert = 61 | Time taken = 12.91034460067749
Number of steps = 50 | Failed to convert = 60 | Time taken = 12.839488983154297
Number of steps = 50 | Failed to convert = 60 | Time taken = 12.815079689025879
Number of steps = 50 | Failed to convert = 60 | Time taken = 12.905352115631104
Number of steps = 50 | Failed to convert = 61 | Time taken = 12.789515256881714
Number of steps = 50 | Failed to convert = 61 | Time taken = 12.771952390670776
Number of steps = 50 | Failed to convert = 59 | Time taken = 12.715665102005005
Number of steps = 50 | Failed to convert = 62 | Time taken = 13.001575231552124
Number of steps = 50 | Failed to convert = 58 | Time taken = 12.657989263534546
Number of steps = 50 | Failed to convert = 52 | Time taken = 9.486287117004395
Number of steps = 50 | Failed to convert = 51 | Time taken = 9.543809413909912
Number of steps = 50 | Failed to convert = 52 | Time taken = 9.44543170928955
Number of steps = 50 | Failed to convert = 51 | Time taken = 9.433141469955444
Number of steps = 50 | Failed to convert = 52 | Time taken = 9.455289602279663
Number of steps = 50 | Failed to convert = 51 | Time taken = 9.52108645439148
Number of steps = 50 | Failed to convert = 53 | Time taken = 9.525346279144287
Number of steps = 50 | Failed to convert = 52 | Time taken = 9.545391321182251
Number of steps = 50 | Failed to convert = 53 | Time taken = 9.60839056968689
Number of steps = 50 | Failed to convert = 52 | Time taken = 9.552079200744629
Number of steps = 50 | Failed to convert = 52 | Time taken = 6.687657117843628
Number of steps = 50 | Failed to convert = 52 | Time taken = 6.742714166641235
Number of steps = 50 | Failed to convert = 51 | Time taken = 6.675718069076538
Number of steps = 50 | Failed to convert = 51 | Time taken = 6.642000198364258
Number of steps = 50 | Failed to convert = 50 | Time taken = 6.626015901565552
Number of steps = 50 | Failed to convert = 51 | Time taken = 6.640568733215332
Number of steps = 50 | Failed to convert = 52 | Time taken = 6.6540679931640625
Number of steps = 50 | Failed to convert = 51 | Time taken = 6.6794116497039795
Number of steps = 50 | Failed to convert = 52 | Time taken = 6.6795783042907715
Number of steps = 50 | Failed to convert = 51 | Time taken = 6.659087657928467
Number of steps = 50 | Failed to convert = 54 | Time taken = 13.13552737236023
Number of steps = 50 | Failed to convert = 58 | Time taken = 13.23125672340393
Number of steps = 50 | Failed to convert = 57 | Time taken = 13.111463069915771
Number of steps = 50 | Failed to convert = 58 | Time taken = 13.265933752059937
Number of steps = 50 | Failed to convert = 60 | Time taken = 13.182299852371216
Number of steps = 50 | Failed to convert = 59 | Time taken = 13.316864013671875
Number of steps = 50 | Failed to convert = 57 | Time taken = 13.214406967163086
Number of steps = 50 | Failed to convert = 57 | Time taken = 13.149035692214966
Number of steps = 50 | Failed to convert = 62 | Time taken = 13.195819616317749
Number of steps = 50 | Failed to convert = 58 | Time taken = 13.189855575561523
Number of steps = 50 | Failed to convert = 55 | Time taken = 9.821985721588135
Number of steps = 50 | Failed to convert = 55 | Time taken = 9.733257293701172
Number of steps = 50 | Failed to convert = 52 | Time taken = 9.796756982803345
Number of steps = 50 | Failed to convert = 53 | Time taken = 9.812137126922607
Number of steps = 50 | Failed to convert = 54 | Time taken = 9.906115770339966
Number of steps = 50 | Failed to convert = 54 | Time taken = 9.829766273498535
Number of steps = 50 | Failed to convert = 52 | Time taken = 9.750823259353638
Number of steps = 50 | Failed to convert = 55 | Time taken = 9.879374742507935
Number of steps = 50 | Failed to convert = 51 | Time taken = 9.867537260055542
Number of steps = 50 | Failed to convert = 53 | Time taken = 9.775737762451172
Number of steps = 50 | Failed to convert = 55 | Time taken = 6.981818199157715
Number of steps = 50 | Failed to convert = 55 | Time taken = 7.007926940917969
Number of steps = 50 | Failed to convert = 55 | Time taken = 6.983127117156982
Number of steps = 50 | Failed to convert = 55 | Time taken = 6.970696210861206
Number of steps = 50 | Failed to convert = 56 | Time taken = 6.983698844909668
Number of steps = 50 | Failed to convert = 51 | Time taken = 6.807557582855225
Number of steps = 50 | Failed to convert = 55 | Time taken = 6.94247579574585
Number of steps = 50 | Failed to convert = 56 | Time taken = 6.850175619125366
Number of steps = 50 | Failed to convert = 56 | Time taken = 6.969822883605957
Number of steps = 50 | Failed to convert = 55 | Time taken = 6.961949586868286
torch.Size([1000, 10, 3])
tensor([[[6.9973e-02, 1.9438e+00, 1.1859e+02],
         [8.4975e-02, 2.1794e+00, 8.9979e+01],
         [7.4994e-02, 2.7112e+00, 9.0669e+01],
         ...,
         [6.4990e-02, 3.0952e+00, 9.2261e+01],
         [7.9988e-02, 2.4856e+00, 9.0860e+01],
         [6.9998e-02, 2.7440e+00, 1.2028e+02]],

        [[1.1000e-01, 3.8975e+00, 1.6458e+02],
         [1.4500e-01, 3.8256e+00, 1.6713e+02],
         [1.3495e-01, 3.4393e+00, 1.4512e+02],
         ...,
         [1.0992e-01, 4.3373e+00, 1.8946e+02],
         [1.1483e-01, 3.6157e+00, 1.6832e+02],
         [1.4496e-01, 3.9309e+00, 1.6871e+02]],

        [[6.9998e-02, 2.2240e+00, 1.2250e+02],
         [5.9957e-02, 2.2296e+00, 8.9794e+01],
         [6.4971e-02, 2.6766e+00, 9.2797e+01],
         ...,
         [7.9998e-02, 2.4985e+00, 8.8063e+01],
         [5.9990e-02, 2.7492e+00, 8.9844e+01],
         [7.4974e-02, 2.4765e+00, 9.0386e+01]],

        ...,

        [[3.9992e-02, 1.1008e+00, 1.7125e+02],
         [2.9989e-02, 8.1636e-01, 9.1495e+01],
         [4.4926e-02, 8.3105e-01, 6.2170e+01],
         ...,
         [3.9993e-02, 5.5204e+00, 6.1393e+01],
         [3.9994e-02, 1.3862e+00, 2.0272e+02],
         [4.4997e-02, 1.1054e+00, 6.1234e+01]],

        [[1.4998e-01, 3.3048e+00, 1.7590e+02],
         [1.2496e-01, 3.7867e+00, 1.5427e+02],
         [1.5500e-01, 4.2532e+00, 2.0754e+02],
         ...,
         [1.4980e-01, 4.0685e+00, 1.4616e+02],
         [1.6482e-01, 4.6239e+00, 1.7693e+02],
         [1.7987e-01, 5.1439e+00, 2.0660e+02]],

        [[1.2995e-01, 4.3780e+00, 1.8242e+02],
         [1.2997e-01, 3.8418e+00, 1.8395e+02],
         [1.2999e-01, 4.4485e+00, 1.8348e+02],
         ...,
         [1.3993e-01, 4.1291e+00, 1.8062e+02],
         [1.2998e-01, 4.1256e+00, 1.8880e+02],
         [1.2500e-01, 4.2741e+00, 1.8278e+02]]])
Getting random attacks
Number of steps = 50 | Failed to convert = 45 | Time taken = 14.211015224456787
Number of steps = 50 | Failed to convert = 44 | Time taken = 14.220580577850342
Number of steps = 50 | Failed to convert = 45 | Time taken = 13.992136240005493
Number of steps = 50 | Failed to convert = 47 | Time taken = 14.017235279083252
Number of steps = 50 | Failed to convert = 42 | Time taken = 13.959602117538452
Number of steps = 50 | Failed to convert = 42 | Time taken = 14.290695428848267
Number of steps = 50 | Failed to convert = 43 | Time taken = 13.986515522003174
Number of steps = 50 | Failed to convert = 46 | Time taken = 14.014328956604004
Number of steps = 50 | Failed to convert = 44 | Time taken = 13.916703462600708
Number of steps = 50 | Failed to convert = 44 | Time taken = 13.885610342025757
Number of steps = 50 | Failed to convert = 39 | Time taken = 9.53187084197998
Number of steps = 50 | Failed to convert = 39 | Time taken = 9.393163204193115
Number of steps = 50 | Failed to convert = 39 | Time taken = 9.447727918624878
Number of steps = 50 | Failed to convert = 40 | Time taken = 9.495716333389282
Number of steps = 50 | Failed to convert = 39 | Time taken = 9.580907106399536
Number of steps = 50 | Failed to convert = 39 | Time taken = 9.373677253723145
Number of steps = 50 | Failed to convert = 38 | Time taken = 9.25094723701477
Number of steps = 50 | Failed to convert = 38 | Time taken = 9.42905306816101
Number of steps = 50 | Failed to convert = 40 | Time taken = 9.772760152816772
Number of steps = 50 | Failed to convert = 37 | Time taken = 9.311720132827759
Number of steps = 50 | Failed to convert = 39 | Time taken = 5.222621917724609
Number of steps = 50 | Failed to convert = 37 | Time taken = 5.057108402252197
Number of steps = 50 | Failed to convert = 37 | Time taken = 5.115200757980347
Number of steps = 50 | Failed to convert = 37 | Time taken = 5.081599235534668
Number of steps = 50 | Failed to convert = 39 | Time taken = 5.135549068450928
Number of steps = 50 | Failed to convert = 38 | Time taken = 5.14714789390564
Number of steps = 50 | Failed to convert = 39 | Time taken = 5.124526023864746
Number of steps = 50 | Failed to convert = 38 | Time taken = 5.124710321426392
Number of steps = 50 | Failed to convert = 38 | Time taken = 5.048988342285156
Number of steps = 50 | Failed to convert = 38 | Time taken = 5.115033388137817
Number of steps = 50 | Failed to convert = 55 | Time taken = 13.964992046356201
Number of steps = 50 | Failed to convert = 52 | Time taken = 14.014070272445679
Number of steps = 50 | Failed to convert = 58 | Time taken = 14.109023809432983
Number of steps = 50 | Failed to convert = 58 | Time taken = 13.959029197692871
Number of steps = 50 | Failed to convert = 56 | Time taken = 13.965753555297852
Number of steps = 50 | Failed to convert = 57 | Time taken = 14.065015077590942
Number of steps = 50 | Failed to convert = 55 | Time taken = 14.061070680618286
Number of steps = 50 | Failed to convert = 56 | Time taken = 14.0934317111969
Number of steps = 50 | Failed to convert = 54 | Time taken = 14.313947200775146
Number of steps = 50 | Failed to convert = 59 | Time taken = 13.81187629699707
Number of steps = 50 | Failed to convert = 51 | Time taken = 10.286620140075684
Number of steps = 50 | Failed to convert = 51 | Time taken = 10.247751474380493
Number of steps = 50 | Failed to convert = 49 | Time taken = 10.244115114212036
Number of steps = 50 | Failed to convert = 51 | Time taken = 10.37276554107666
Number of steps = 50 | Failed to convert = 50 | Time taken = 10.256896257400513
Number of steps = 50 | Failed to convert = 50 | Time taken = 10.308010339736938
Number of steps = 50 | Failed to convert = 51 | Time taken = 10.295977592468262
Number of steps = 50 | Failed to convert = 50 | Time taken = 10.147035598754883
Number of steps = 50 | Failed to convert = 50 | Time taken = 10.235770225524902
Number of steps = 50 | Failed to convert = 50 | Time taken = 10.255295991897583
Number of steps = 50 | Failed to convert = 51 | Time taken = 6.886999845504761
Number of steps = 50 | Failed to convert = 48 | Time taken = 6.747981071472168
Number of steps = 50 | Failed to convert = 49 | Time taken = 6.843947649002075
Number of steps = 50 | Failed to convert = 51 | Time taken = 6.909499883651733
Number of steps = 50 | Failed to convert = 51 | Time taken = 7.024440765380859
Number of steps = 50 | Failed to convert = 49 | Time taken = 6.91004490852356
Number of steps = 50 | Failed to convert = 49 | Time taken = 6.933879852294922
Number of steps = 50 | Failed to convert = 50 | Time taken = 6.899175643920898
Number of steps = 50 | Failed to convert = 48 | Time taken = 6.795985221862793
Number of steps = 50 | Failed to convert = 50 | Time taken = 6.871277332305908
torch.Size([1000, 10, 3])
tensor([[[2.5432e-01, 1.2962e+01, 9.4493e+02],
         [2.5493e-01, 1.2854e+01, 9.4399e+02],
         [2.5498e-01, 1.3107e+01, 9.5934e+02],
         ...,
         [2.5491e-01, 1.2634e+01, 9.5151e+02],
         [2.5495e-01, 1.2414e+01, 9.4828e+02],
         [2.5468e-01, 1.3119e+01, 9.4384e+02]],

        [[5.4982e-02, 1.6343e+00, 6.0836e+01],
         [4.9999e-02, 1.4145e+00, 5.9329e+01],
         [5.4936e-02, 1.8954e+00, 6.2404e+01],
         ...,
         [4.4988e-02, 1.3732e+00, 6.1197e+01],
         [4.9995e-02, 1.9325e+00, 9.1872e+01],
         [4.9988e-02, 1.8878e+00, 6.1924e+01]],

        [[4.9985e-02, 1.3652e+00, 8.7650e+01],
         [9.4928e-02, 1.6130e+00, 8.9629e+01],
         [4.4980e-02, 1.8834e+00, 6.0317e+01],
         ...,
         [6.9906e-02, 2.6380e+00, 8.6973e+01],
         [8.9995e-02, 1.3248e+00, 5.9230e+01],
         [6.4948e-02, 2.4392e+00, 8.7179e+01]],

        ...,

        [[2.0990e-01, 8.5936e+00, 4.0570e+02],
         [2.4494e-01, 7.2180e+00, 3.2667e+02],
         [2.0985e-01, 6.5576e+00, 3.2932e+02],
         ...,
         [1.9993e-01, 8.0524e+00, 3.0673e+02],
         [1.8493e-01, 6.7921e+00, 3.0198e+02],
         [2.2998e-01, 6.8823e+00, 2.8219e+02]],

        [[7.4933e-02, 2.5014e+00, 1.2502e+02],
         [7.4990e-02, 2.5296e+00, 1.2559e+02],
         [8.4976e-02, 2.7470e+00, 9.3070e+01],
         ...,
         [7.9942e-02, 2.5464e+00, 8.9800e+01],
         [8.9998e-02, 2.4832e+00, 1.2260e+02],
         [7.4938e-02, 2.8072e+00, 1.2418e+02]],

        [[1.2492e-01, 3.4792e+00, 1.7426e+02],
         [1.1497e-01, 3.7798e+00, 1.4652e+02],
         [9.9909e-02, 3.5352e+00, 1.4586e+02],
         ...,
         [1.2499e-01, 3.7446e+00, 2.0741e+02],
         [1.0999e-01, 3.2220e+00, 1.7527e+02],
         [1.2500e-01, 3.2678e+00, 1.7253e+02]]])
Time taken: 1232.90 s
# 'independent'
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=0, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='rand', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='independent', model_dataset='CIFAR10', model_id='independent', model_normalize=0, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_independent_model-unnormalized_data-unnormalized
File Directory: ./files/CIFAR10/model_independent_model-unnormalized_data-unnormalized
cuda:0
22-10-10 03:02
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight". 
	size mismatch for module.block1.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 16, 3, 3]).
	size mismatch for module.block1.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([320, 160, 1, 1]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([640, 320, 1, 1]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.batch_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.fc.weight: copying a param with shape torch.Size([10, 64]) from checkpoint, the shape in current model is torch.Size([10, 640]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.batch_norm.num_batches_tracked", "module.fc.weight", "module.fc.bias". 
# C.2 'pre-act-18' | Different architecture
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=0, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='rand', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='pre-act-18', model_dataset='CIFAR10', model_id='pre-act-18', model_normalize=0, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_pre-act-18_model-unnormalized_data-unnormalized
File Directory: ./files/CIFAR10/model_pre-act-18_model-unnormalized_data-unnormalized
cuda:0
22-10-10 03:02
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.fc.weight", "module.fc.bias". 
	Unexpected key(s) in state_dict: "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
	size mismatch for module.conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
# --model_normalize 0, --data_normalize 1
# Teacher/Source/Victim model
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=1, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='rand', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='teacher', model_dataset='CIFAR10', model_id='teacher', model_normalize=0, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_teacher_model-unnormalized_data-normalized
File Directory: ./files/CIFAR10/model_teacher_model-unnormalized_data-normalized
cuda:0
22-10-10 03:02
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Model: ./models/CIFAR10/model_teacher_model-unnormalized_data-normalized | 	 Test Acc: 0.377
Getting random attacks
Number of steps = 50 | Failed to convert = 16 | Time taken = 5.129419565200806
Number of steps = 50 | Failed to convert = 18 | Time taken = 5.216142416000366
Number of steps = 50 | Failed to convert = 21 | Time taken = 5.017282724380493
Number of steps = 50 | Failed to convert = 18 | Time taken = 5.0229575634002686
Number of steps = 50 | Failed to convert = 15 | Time taken = 5.117224216461182
Number of steps = 50 | Failed to convert = 21 | Time taken = 5.0267980098724365
Number of steps = 50 | Failed to convert = 18 | Time taken = 5.259028673171997
Number of steps = 50 | Failed to convert = 14 | Time taken = 5.100353717803955
Number of steps = 50 | Failed to convert = 12 | Time taken = 5.027754306793213
Number of steps = 50 | Failed to convert = 18 | Time taken = 4.865667819976807
Number of steps = 50 | Failed to convert = 4 | Time taken = 3.51747727394104
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.583611249923706
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.707131862640381
Number of steps = 50 | Failed to convert = 8 | Time taken = 3.5661070346832275
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.680324077606201
Number of steps = 50 | Failed to convert = 8 | Time taken = 3.745129346847534
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.785837173461914
Number of steps = 50 | Failed to convert = 9 | Time taken = 3.6232192516326904
Number of steps = 50 | Failed to convert = 5 | Time taken = 3.623133659362793
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.521315097808838
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.2444002628326416
Number of steps = 50 | Failed to convert = 7 | Time taken = 2.29179310798645
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.213484287261963
Number of steps = 50 | Failed to convert = 6 | Time taken = 2.272472620010376
Number of steps = 50 | Failed to convert = 7 | Time taken = 2.265591621398926
Number of steps = 50 | Failed to convert = 4 | Time taken = 2.091535806655884
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.165159225463867
Number of steps = 50 | Failed to convert = 9 | Time taken = 2.453620195388794
Number of steps = 50 | Failed to convert = 7 | Time taken = 2.2663352489471436
Number of steps = 50 | Failed to convert = 6 | Time taken = 2.248314142227173
Number of steps = 50 | Failed to convert = 16 | Time taken = 4.808369398117065
Number of steps = 50 | Failed to convert = 13 | Time taken = 4.857162714004517
Number of steps = 50 | Failed to convert = 15 | Time taken = 4.932043790817261
Number of steps = 50 | Failed to convert = 12 | Time taken = 4.72594428062439
Number of steps = 50 | Failed to convert = 16 | Time taken = 4.929136037826538
Number of steps = 50 | Failed to convert = 14 | Time taken = 4.805334568023682
Number of steps = 50 | Failed to convert = 11 | Time taken = 4.7344255447387695
Number of steps = 50 | Failed to convert = 12 | Time taken = 4.611754894256592
Number of steps = 50 | Failed to convert = 18 | Time taken = 4.801589727401733
Number of steps = 50 | Failed to convert = 12 | Time taken = 4.730647563934326
Number of steps = 50 | Failed to convert = 3 | Time taken = 3.3737823963165283
Number of steps = 50 | Failed to convert = 7 | Time taken = 3.606692314147949
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.464679718017578
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.4250476360321045
Number of steps = 50 | Failed to convert = 4 | Time taken = 3.5018630027770996
Number of steps = 50 | Failed to convert = 5 | Time taken = 3.529590129852295
Number of steps = 50 | Failed to convert = 5 | Time taken = 3.5148115158081055
Number of steps = 50 | Failed to convert = 5 | Time taken = 3.464235544204712
Number of steps = 50 | Failed to convert = 5 | Time taken = 3.545281410217285
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.472012996673584
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.1515207290649414
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.0850470066070557
Number of steps = 50 | Failed to convert = 6 | Time taken = 2.14387583732605
Number of steps = 50 | Failed to convert = 4 | Time taken = 2.0988218784332275
Number of steps = 50 | Failed to convert = 9 | Time taken = 2.2658541202545166
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.1244168281555176
Number of steps = 50 | Failed to convert = 3 | Time taken = 1.9974353313446045
Number of steps = 50 | Failed to convert = 6 | Time taken = 2.177560806274414
Number of steps = 50 | Failed to convert = 6 | Time taken = 2.1464779376983643
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.134671688079834
torch.Size([1000, 10, 3])
tensor([[[2.3991e-01, 7.5828e+00, 2.4152e+02],
         [2.2493e-01, 7.9368e+00, 3.0978e+02],
         [2.3998e-01, 5.6382e+00, 2.0812e+02],
         ...,
         [1.4498e-01, 7.9252e+00, 2.3693e+02],
         [2.0997e-01, 8.2802e+00, 2.6778e+02],
         [2.3999e-01, 6.9909e+00, 2.6342e+02]],

        [[1.4999e-01, 4.1338e+00, 1.9625e+02],
         [1.4500e-01, 4.5218e+00, 1.9025e+02],
         [1.5495e-01, 4.3998e+00, 1.8538e+02],
         ...,
         [1.3491e-01, 3.8708e+00, 1.8928e+02],
         [1.6476e-01, 4.0887e+00, 1.7070e+02],
         [1.4996e-01, 4.4069e+00, 1.6822e+02]],

        [[1.2500e-01, 3.8470e+00, 1.7927e+02],
         [1.0992e-01, 4.1214e+00, 1.8161e+02],
         [1.2495e-01, 4.7149e+00, 1.4900e+02],
         ...,
         [1.4000e-01, 3.8358e+00, 1.7895e+02],
         [1.3498e-01, 4.3329e+00, 1.8097e+02],
         [1.2496e-01, 4.0675e+00, 1.8052e+02]],

        ...,

        [[4.9989e-03, 2.7578e-01, 2.9829e+01],
         [4.9982e-03, 2.7255e-01, 3.0127e+01],
         [4.9918e-03, 2.7714e-01, 3.0247e+01],
         ...,
         [4.9991e-03, 2.7513e-01, 3.0142e+01],
         [4.9993e-03, 2.7831e-01, 3.0454e+01],
         [4.9996e-03, 2.7679e-01, 3.0248e+01]],

        [[2.5496e-01, 4.3766e+00, 2.9319e+02],
         [2.5492e-01, 9.7419e+00, 2.4474e+02],
         [2.5500e-01, 8.1737e+00, 2.6255e+02],
         ...,
         [1.6478e-01, 9.4647e+00, 2.0961e+02],
         [2.5472e-01, 8.1696e+00, 2.4068e+02],
         [2.2983e-01, 7.4321e+00, 3.5821e+02]],

        [[9.4961e-02, 3.5613e+00, 1.2168e+02],
         [9.4981e-02, 3.5693e+00, 1.5541e+02],
         [8.9991e-02, 3.3471e+00, 1.5128e+02],
         ...,
         [8.4956e-02, 3.5812e+00, 1.5505e+02],
         [8.4987e-02, 3.5794e+00, 1.8331e+02],
         [9.4997e-02, 3.1392e+00, 1.2172e+02]]])
Getting random attacks
Number of steps = 50 | Failed to convert = 13 | Time taken = 4.497715711593628
Number of steps = 50 | Failed to convert = 15 | Time taken = 4.696567058563232
Number of steps = 50 | Failed to convert = 15 | Time taken = 4.515535116195679
Number of steps = 50 | Failed to convert = 17 | Time taken = 4.6607255935668945
Number of steps = 50 | Failed to convert = 16 | Time taken = 4.590081214904785
Number of steps = 50 | Failed to convert = 15 | Time taken = 4.547329425811768
Number of steps = 50 | Failed to convert = 20 | Time taken = 4.620702266693115
Number of steps = 50 | Failed to convert = 14 | Time taken = 4.506811857223511
Number of steps = 50 | Failed to convert = 16 | Time taken = 4.593536853790283
Number of steps = 50 | Failed to convert = 17 | Time taken = 4.446898460388184
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.3317582607269287
Number of steps = 50 | Failed to convert = 5 | Time taken = 3.283930540084839
Number of steps = 50 | Failed to convert = 7 | Time taken = 3.27272367477417
Number of steps = 50 | Failed to convert = 5 | Time taken = 3.240048408508301
Number of steps = 50 | Failed to convert = 7 | Time taken = 3.319826364517212
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.188420057296753
Number of steps = 50 | Failed to convert = 4 | Time taken = 3.1633713245391846
Number of steps = 50 | Failed to convert = 5 | Time taken = 3.381237268447876
Number of steps = 50 | Failed to convert = 4 | Time taken = 3.3024375438690186
Number of steps = 50 | Failed to convert = 4 | Time taken = 3.28851318359375
Number of steps = 50 | Failed to convert = 2 | Time taken = 1.9551286697387695
Number of steps = 50 | Failed to convert = 2 | Time taken = 1.9554340839385986
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.070075750350952
Number of steps = 50 | Failed to convert = 2 | Time taken = 1.9455790519714355
Number of steps = 50 | Failed to convert = 3 | Time taken = 1.981539011001587
Number of steps = 50 | Failed to convert = 3 | Time taken = 1.9903576374053955
Number of steps = 50 | Failed to convert = 4 | Time taken = 1.9979569911956787
Number of steps = 50 | Failed to convert = 3 | Time taken = 1.9667911529541016
Number of steps = 50 | Failed to convert = 4 | Time taken = 1.9537084102630615
Number of steps = 50 | Failed to convert = 4 | Time taken = 1.9587759971618652
Number of steps = 50 | Failed to convert = 20 | Time taken = 4.681298732757568
Number of steps = 50 | Failed to convert = 13 | Time taken = 4.51877760887146
Number of steps = 50 | Failed to convert = 21 | Time taken = 4.634251832962036
Number of steps = 50 | Failed to convert = 11 | Time taken = 4.665408611297607
Number of steps = 50 | Failed to convert = 11 | Time taken = 4.499191761016846
Number of steps = 50 | Failed to convert = 16 | Time taken = 4.651729583740234
Number of steps = 50 | Failed to convert = 17 | Time taken = 4.60559606552124
Number of steps = 50 | Failed to convert = 17 | Time taken = 4.512834310531616
Number of steps = 50 | Failed to convert = 13 | Time taken = 4.65351414680481
Number of steps = 50 | Failed to convert = 16 | Time taken = 4.5339953899383545
Number of steps = 50 | Failed to convert = 4 | Time taken = 3.3100528717041016
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.2783191204071045
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.378539562225342
Number of steps = 50 | Failed to convert = 7 | Time taken = 3.350919008255005
Number of steps = 50 | Failed to convert = 4 | Time taken = 3.2429141998291016
Number of steps = 50 | Failed to convert = 5 | Time taken = 3.1863694190979004
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.2542166709899902
Number of steps = 50 | Failed to convert = 4 | Time taken = 3.2714595794677734
Number of steps = 50 | Failed to convert = 9 | Time taken = 3.4324843883514404
Number of steps = 50 | Failed to convert = 9 | Time taken = 3.373972177505493
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.0872080326080322
Number of steps = 50 | Failed to convert = 2 | Time taken = 1.967890977859497
Number of steps = 50 | Failed to convert = 2 | Time taken = 1.9724109172821045
Number of steps = 28 | Failed to convert = 0 | Time taken = 1.748215913772583
Number of steps = 50 | Failed to convert = 4 | Time taken = 2.028398036956787
Number of steps = 50 | Failed to convert = 3 | Time taken = 1.9704692363739014
Number of steps = 50 | Failed to convert = 3 | Time taken = 1.9539635181427002
Number of steps = 50 | Failed to convert = 4 | Time taken = 1.9974348545074463
Number of steps = 50 | Failed to convert = 2 | Time taken = 1.9631366729736328
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.0682201385498047
torch.Size([1000, 10, 3])
tensor([[[4.9866e-03, 2.8191e-01, 3.0616e+01],
         [4.9986e-03, 2.7315e-01, 3.0347e+01],
         [4.9995e-03, 2.7907e-01, 3.1253e+01],
         ...,
         [4.9983e-03, 2.7236e-01, 3.0863e+01],
         [4.9990e-03, 2.6929e-01, 3.1026e+01],
         [4.9938e-03, 2.8034e-01, 3.0831e+01]],

        [[4.9984e-03, 2.7238e-01, 3.0495e+01],
         [4.9999e-03, 2.8290e-01, 3.1666e+01],
         [4.9942e-03, 2.7077e-01, 2.9983e+01],
         ...,
         [4.9986e-03, 2.7464e-01, 3.0929e+01],
         [4.9995e-03, 2.7607e-01, 3.0671e+01],
         [4.9988e-03, 2.6969e-01, 3.0455e+01]],

        [[4.9985e-03, 2.7642e-01, 3.0390e+01],
         [4.9962e-03, 2.7441e-01, 2.9890e+01],
         [4.9977e-03, 2.7508e-01, 2.9651e+01],
         ...,
         [4.9933e-03, 2.7140e-01, 3.0622e+01],
         [4.9997e-03, 2.6900e-01, 3.1036e+01],
         [4.9960e-03, 2.7856e-01, 2.9962e+01]],

        ...,

        [[4.9975e-03, 2.7478e-01, 3.1162e+01],
         [4.9989e-03, 2.7998e-01, 3.1180e+01],
         [4.9964e-03, 2.7583e-01, 3.0141e+01],
         ...,
         [4.9983e-03, 2.8132e-01, 3.0261e+01],
         [4.9981e-03, 2.7439e-01, 3.2427e+01],
         [4.9996e-03, 2.7862e-01, 3.0438e+01]],

        [[4.9956e-03, 2.7845e-01, 3.0340e+01],
         [4.9993e-03, 2.8149e-01, 3.0363e+01],
         [4.9986e-03, 2.7513e-01, 3.1775e+01],
         ...,
         [4.9964e-03, 2.8299e-01, 3.1097e+01],
         [4.9999e-03, 2.7600e-01, 3.0304e+01],
         [4.9959e-03, 2.8086e-01, 3.0596e+01]],

        [[4.9967e-03, 2.7477e-01, 3.0269e+01],
         [4.9986e-03, 2.7845e-01, 3.0270e+01],
         [4.9955e-03, 2.7910e-01, 3.0894e+01],
         ...,
         [4.9996e-03, 2.7717e-01, 3.0432e+01],
         [4.9998e-03, 2.7551e-01, 3.1716e+01],
         [4.9998e-03, 2.7947e-01, 3.0123e+01]]])
Time taken: 455.75 s
# 'independent'
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=1, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='rand', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='independent', model_dataset='CIFAR10', model_id='independent', model_normalize=0, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_independent_model-unnormalized_data-normalized
File Directory: ./files/CIFAR10/model_independent_model-unnormalized_data-normalized
cuda:0
22-10-10 03:10
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight". 
	size mismatch for module.block1.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 16, 3, 3]).
	size mismatch for module.block1.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([320, 160, 1, 1]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([640, 320, 1, 1]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.batch_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.fc.weight: copying a param with shape torch.Size([10, 64]) from checkpoint, the shape in current model is torch.Size([10, 640]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.batch_norm.num_batches_tracked", "module.fc.weight", "module.fc.bias". 
# C.2 'pre-act-18' | Different architecture
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=1, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='rand', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='pre-act-18', model_dataset='CIFAR10', model_id='pre-act-18', model_normalize=0, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_pre-act-18_model-unnormalized_data-normalized
File Directory: ./files/CIFAR10/model_pre-act-18_model-unnormalized_data-normalized
cuda:0
22-10-10 03:10
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.fc.weight", "module.fc.bias". 
	Unexpected key(s) in state_dict: "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
	size mismatch for module.conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
# --model_normalize 1, --data_normalize 0
# Teacher/Source/Victim model
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=0, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='rand', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='teacher', model_dataset='CIFAR10', model_id='teacher', model_normalize=1, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_teacher_model-normalized_data-unnormalized
File Directory: ./files/CIFAR10/model_teacher_model-normalized_data-unnormalized
cuda:0
22-10-10 03:10
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Model: ./models/CIFAR10/model_teacher_model-normalized_data-unnormalized | 	 Test Acc: 0.942
Getting random attacks
Number of steps = 50 | Failed to convert = 59 | Time taken = 12.898080110549927
Number of steps = 50 | Failed to convert = 57 | Time taken = 13.048391580581665
Number of steps = 50 | Failed to convert = 61 | Time taken = 13.177079439163208
Number of steps = 50 | Failed to convert = 60 | Time taken = 13.04179310798645
Number of steps = 50 | Failed to convert = 56 | Time taken = 13.056083917617798
Number of steps = 50 | Failed to convert = 59 | Time taken = 12.923405647277832
Number of steps = 50 | Failed to convert = 59 | Time taken = 13.012119770050049
Number of steps = 50 | Failed to convert = 56 | Time taken = 12.931642055511475
Number of steps = 50 | Failed to convert = 60 | Time taken = 13.095186471939087
Number of steps = 50 | Failed to convert = 58 | Time taken = 13.066015243530273
Number of steps = 50 | Failed to convert = 54 | Time taken = 9.681838035583496
Number of steps = 50 | Failed to convert = 51 | Time taken = 9.626117467880249
Number of steps = 50 | Failed to convert = 52 | Time taken = 9.633974552154541
Number of steps = 50 | Failed to convert = 54 | Time taken = 9.777586460113525
Number of steps = 50 | Failed to convert = 54 | Time taken = 9.56859302520752
Number of steps = 50 | Failed to convert = 51 | Time taken = 9.661779403686523
Number of steps = 50 | Failed to convert = 53 | Time taken = 9.495879411697388
Number of steps = 50 | Failed to convert = 54 | Time taken = 9.693862915039062
Number of steps = 50 | Failed to convert = 52 | Time taken = 9.772112846374512
Number of steps = 50 | Failed to convert = 54 | Time taken = 9.847884178161621
Number of steps = 50 | Failed to convert = 52 | Time taken = 6.825562238693237
Number of steps = 50 | Failed to convert = 51 | Time taken = 6.82535719871521
Number of steps = 50 | Failed to convert = 52 | Time taken = 6.724749565124512
Number of steps = 50 | Failed to convert = 53 | Time taken = 6.823907375335693
Number of steps = 50 | Failed to convert = 50 | Time taken = 6.771897077560425
Number of steps = 50 | Failed to convert = 53 | Time taken = 6.698021411895752
Number of steps = 50 | Failed to convert = 52 | Time taken = 6.83990740776062
Number of steps = 50 | Failed to convert = 50 | Time taken = 6.737462997436523
Number of steps = 50 | Failed to convert = 52 | Time taken = 6.849244832992554
Number of steps = 50 | Failed to convert = 52 | Time taken = 6.837280511856079
Number of steps = 50 | Failed to convert = 56 | Time taken = 13.389359951019287
Number of steps = 50 | Failed to convert = 59 | Time taken = 13.508278846740723
Number of steps = 50 | Failed to convert = 61 | Time taken = 13.28896951675415
Number of steps = 50 | Failed to convert = 62 | Time taken = 13.518094062805176
Number of steps = 50 | Failed to convert = 59 | Time taken = 13.121564626693726
Number of steps = 50 | Failed to convert = 58 | Time taken = 13.411054372787476
Number of steps = 50 | Failed to convert = 62 | Time taken = 13.449224948883057
Number of steps = 50 | Failed to convert = 58 | Time taken = 13.213927984237671
Number of steps = 50 | Failed to convert = 59 | Time taken = 13.404645919799805
Number of steps = 50 | Failed to convert = 59 | Time taken = 13.17325472831726
Number of steps = 50 | Failed to convert = 54 | Time taken = 10.048418760299683
Number of steps = 50 | Failed to convert = 57 | Time taken = 9.141956329345703
Number of steps = 50 | Failed to convert = 54 | Time taken = 10.130602359771729
Number of steps = 50 | Failed to convert = 57 | Time taken = 9.513813018798828
Number of steps = 50 | Failed to convert = 55 | Time taken = 9.703100681304932
Number of steps = 50 | Failed to convert = 56 | Time taken = 9.38395619392395
Number of steps = 50 | Failed to convert = 54 | Time taken = 9.964757442474365
Number of steps = 50 | Failed to convert = 57 | Time taken = 9.097575664520264
Number of steps = 50 | Failed to convert = 53 | Time taken = 10.00381326675415
Number of steps = 50 | Failed to convert = 56 | Time taken = 9.748570680618286
Number of steps = 50 | Failed to convert = 56 | Time taken = 7.070240497589111
Number of steps = 50 | Failed to convert = 55 | Time taken = 6.898333549499512
Number of steps = 50 | Failed to convert = 54 | Time taken = 6.957836389541626
Number of steps = 50 | Failed to convert = 56 | Time taken = 6.977421760559082
Number of steps = 50 | Failed to convert = 52 | Time taken = 6.816899061203003
Number of steps = 50 | Failed to convert = 54 | Time taken = 6.743474006652832
Number of steps = 50 | Failed to convert = 56 | Time taken = 6.6399993896484375
Number of steps = 50 | Failed to convert = 54 | Time taken = 6.976762533187866
Number of steps = 50 | Failed to convert = 55 | Time taken = 6.942170858383179
Number of steps = 50 | Failed to convert = 55 | Time taken = 6.960798978805542
torch.Size([1000, 10, 3])
tensor([[[6.4975e-02, 2.4979e+00, 9.2698e+01],
         [7.4978e-02, 2.4517e+00, 1.2250e+02],
         [8.4994e-02, 2.4403e+00, 1.2419e+02],
         ...,
         [6.9990e-02, 2.8151e+00, 9.5260e+01],
         [7.4989e-02, 2.4856e+00, 1.1991e+02],
         [7.4997e-02, 2.4709e+00, 9.0762e+01]],

        [[9.9996e-02, 3.6565e+00, 1.4538e+02],
         [1.3000e-01, 3.5916e+00, 1.7452e+02],
         [1.2995e-01, 3.1930e+00, 1.6960e+02],
         ...,
         [1.0493e-01, 3.6325e+00, 1.6517e+02],
         [1.1483e-01, 3.6157e+00, 1.6961e+02],
         [1.1997e-01, 3.6907e+00, 1.4336e+02]],

        [[6.9998e-02, 2.2240e+00, 9.0986e+01],
         [6.4953e-02, 2.5042e+00, 1.2044e+02],
         [7.4967e-02, 2.6766e+00, 1.2176e+02],
         ...,
         [7.4998e-02, 2.7700e+00, 1.1714e+02],
         [6.9988e-02, 2.4788e+00, 8.9418e+01],
         [7.4974e-02, 2.7461e+00, 9.3257e+01]],

        ...,

        [[4.9989e-03, 2.7578e-01, 3.0872e+01],
         [4.9982e-03, 2.7255e-01, 3.0953e+01],
         [4.9918e-03, 2.7714e-01, 3.0446e+01],
         ...,
         [4.9991e-03, 2.7513e-01, 2.9961e+01],
         [4.9993e-03, 2.7831e-01, 3.0113e+01],
         [4.9996e-03, 2.7679e-01, 2.9926e+01]],

        [[1.1998e-01, 2.7606e+00, 1.8041e+02],
         [1.0997e-01, 3.5214e+00, 1.5477e+02],
         [1.2000e-01, 3.4788e+00, 1.5261e+02],
         ...,
         [1.3482e-01, 3.5375e+00, 1.4924e+02],
         [1.1987e-01, 4.0904e+00, 1.4819e+02],
         [1.3490e-01, 4.3584e+00, 1.4957e+02]],

        [[1.2995e-01, 4.6482e+00, 1.8346e+02],
         [1.3497e-01, 3.8418e+00, 1.8524e+02],
         [1.3499e-01, 4.7206e+00, 1.8042e+02],
         ...,
         [1.3993e-01, 4.3993e+00, 2.4388e+02],
         [1.3998e-01, 4.1256e+00, 1.8227e+02],
         [1.2500e-01, 4.5551e+00, 1.8101e+02]]])
Getting random attacks
Number of steps = 50 | Failed to convert = 45 | Time taken = 14.284729242324829
Number of steps = 50 | Failed to convert = 42 | Time taken = 14.414570093154907
Number of steps = 50 | Failed to convert = 48 | Time taken = 14.41959547996521
Number of steps = 50 | Failed to convert = 46 | Time taken = 14.269178628921509
Number of steps = 50 | Failed to convert = 46 | Time taken = 14.226853847503662
Number of steps = 50 | Failed to convert = 43 | Time taken = 14.445594549179077
Number of steps = 50 | Failed to convert = 46 | Time taken = 14.398795127868652
Number of steps = 50 | Failed to convert = 46 | Time taken = 14.304705381393433
Number of steps = 50 | Failed to convert = 42 | Time taken = 14.003299951553345
Number of steps = 50 | Failed to convert = 44 | Time taken = 14.274906635284424
Number of steps = 50 | Failed to convert = 38 | Time taken = 9.737268447875977
Number of steps = 50 | Failed to convert = 36 | Time taken = 9.660555124282837
Number of steps = 50 | Failed to convert = 40 | Time taken = 9.608932495117188
Number of steps = 50 | Failed to convert = 39 | Time taken = 9.743375301361084
Number of steps = 50 | Failed to convert = 39 | Time taken = 9.579898357391357
Number of steps = 50 | Failed to convert = 39 | Time taken = 9.650022745132446
Number of steps = 50 | Failed to convert = 39 | Time taken = 9.595612049102783
Number of steps = 50 | Failed to convert = 39 | Time taken = 9.754928827285767
Number of steps = 50 | Failed to convert = 39 | Time taken = 9.869083881378174
Number of steps = 50 | Failed to convert = 40 | Time taken = 9.816421508789062
Number of steps = 50 | Failed to convert = 37 | Time taken = 5.104155778884888
Number of steps = 50 | Failed to convert = 37 | Time taken = 5.216377258300781
Number of steps = 50 | Failed to convert = 34 | Time taken = 5.091763257980347
Number of steps = 50 | Failed to convert = 37 | Time taken = 5.176079034805298
Number of steps = 50 | Failed to convert = 37 | Time taken = 5.149204730987549
Number of steps = 50 | Failed to convert = 38 | Time taken = 5.205821514129639
Number of steps = 50 | Failed to convert = 37 | Time taken = 5.212193489074707
Number of steps = 50 | Failed to convert = 38 | Time taken = 5.112764358520508
Number of steps = 50 | Failed to convert = 37 | Time taken = 5.208428859710693
Number of steps = 50 | Failed to convert = 38 | Time taken = 5.12274956703186
Number of steps = 50 | Failed to convert = 56 | Time taken = 14.387278079986572
Number of steps = 50 | Failed to convert = 55 | Time taken = 14.298582792282104
Number of steps = 50 | Failed to convert = 63 | Time taken = 14.369911670684814
Number of steps = 50 | Failed to convert = 58 | Time taken = 14.368447542190552
Number of steps = 50 | Failed to convert = 55 | Time taken = 14.334747314453125
Number of steps = 50 | Failed to convert = 55 | Time taken = 14.23865270614624
Number of steps = 50 | Failed to convert = 54 | Time taken = 14.560003995895386
Number of steps = 50 | Failed to convert = 58 | Time taken = 14.300079345703125
Number of steps = 50 | Failed to convert = 59 | Time taken = 14.282819986343384
Number of steps = 50 | Failed to convert = 54 | Time taken = 14.268103122711182
Number of steps = 50 | Failed to convert = 50 | Time taken = 10.336119651794434
Number of steps = 50 | Failed to convert = 49 | Time taken = 10.328661680221558
Number of steps = 50 | Failed to convert = 50 | Time taken = 10.37459111213684
Number of steps = 50 | Failed to convert = 49 | Time taken = 10.51430606842041
Number of steps = 50 | Failed to convert = 49 | Time taken = 10.365877866744995
Number of steps = 50 | Failed to convert = 49 | Time taken = 10.460096836090088
Number of steps = 50 | Failed to convert = 51 | Time taken = 10.507033586502075
Number of steps = 50 | Failed to convert = 51 | Time taken = 10.369771480560303
Number of steps = 50 | Failed to convert = 49 | Time taken = 10.440237283706665
Number of steps = 50 | Failed to convert = 51 | Time taken = 10.416409969329834
Number of steps = 50 | Failed to convert = 51 | Time taken = 7.0426576137542725
Number of steps = 50 | Failed to convert = 51 | Time taken = 7.00034499168396
Number of steps = 50 | Failed to convert = 51 | Time taken = 7.024697303771973
Number of steps = 50 | Failed to convert = 51 | Time taken = 7.007519960403442
Number of steps = 50 | Failed to convert = 50 | Time taken = 6.989711761474609
Number of steps = 50 | Failed to convert = 51 | Time taken = 7.052534103393555
Number of steps = 50 | Failed to convert = 49 | Time taken = 7.008143663406372
Number of steps = 50 | Failed to convert = 48 | Time taken = 6.862096309661865
Number of steps = 50 | Failed to convert = 51 | Time taken = 6.995385646820068
Number of steps = 50 | Failed to convert = 51 | Time taken = 7.038654088973999
torch.Size([1000, 10, 3])
tensor([[[2.5432e-01, 1.2962e+01, 9.2824e+02],
         [2.5493e-01, 1.2854e+01, 9.3800e+02],
         [2.5498e-01, 1.3107e+01, 9.2200e+02],
         ...,
         [2.5491e-01, 1.2634e+01, 9.1992e+02],
         [2.5495e-01, 1.2414e+01, 9.5132e+02],
         [2.5468e-01, 1.3119e+01, 9.2979e+02]],

        [[3.4989e-02, 1.3619e+00, 6.1136e+01],
         [4.4999e-02, 1.4145e+00, 5.9801e+01],
         [3.9954e-02, 1.3539e+00, 6.0461e+01],
         ...,
         [2.9992e-02, 1.0985e+00, 6.1625e+01],
         [4.4996e-02, 1.6564e+00, 6.2732e+01],
         [3.9991e-02, 1.3485e+00, 6.0562e+01]],

        [[3.9988e-02, 1.3652e+00, 5.8823e+01],
         [7.4943e-02, 1.3484e+00, 5.9529e+01],
         [3.9982e-02, 1.3524e+00, 5.7757e+01],
         ...,
         [5.9920e-02, 2.1172e+00, 8.7812e+01],
         [6.4996e-02, 1.3248e+00, 6.0203e+01],
         [5.4956e-02, 2.1728e+00, 5.9771e+01]],

        ...,

        [[1.8491e-01, 6.5343e+00, 2.4275e+02],
         [1.7496e-01, 6.6791e+00, 2.4497e+02],
         [1.9486e-01, 4.9490e+00, 2.3862e+02],
         ...,
         [1.7994e-01, 5.0585e+00, 2.4461e+02],
         [1.6993e-01, 5.7385e+00, 2.8063e+02],
         [1.9998e-01, 6.3507e+00, 2.1159e+02]],

        [[6.9938e-02, 2.5014e+00, 1.2328e+02],
         [6.9990e-02, 2.5296e+00, 1.2417e+02],
         [6.9980e-02, 2.7470e+00, 1.2541e+02],
         ...,
         [6.9949e-02, 2.5464e+00, 1.2404e+02],
         [7.4999e-02, 2.2078e+00, 1.2127e+02],
         [7.4938e-02, 2.5266e+00, 1.2254e+02]],

        [[1.3491e-01, 3.9910e+00, 1.7949e+02],
         [1.3496e-01, 4.0361e+00, 1.7576e+02],
         [1.2489e-01, 3.7985e+00, 1.7653e+02],
         ...,
         [1.2499e-01, 3.9984e+00, 1.4865e+02],
         [1.1999e-01, 3.9933e+00, 1.7809e+02],
         [1.4000e-01, 3.7897e+00, 1.7292e+02]]])
Time taken: 1248.70 s
# 'independent'
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=0, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='rand', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='independent', model_dataset='CIFAR10', model_id='independent', model_normalize=1, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_independent_model-normalized_data-unnormalized
File Directory: ./files/CIFAR10/model_independent_model-normalized_data-unnormalized
cuda:0
22-10-10 03:31
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight". 
	size mismatch for module.block1.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 16, 3, 3]).
	size mismatch for module.block1.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([320, 160, 1, 1]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([640, 320, 1, 1]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.batch_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.fc.weight: copying a param with shape torch.Size([10, 64]) from checkpoint, the shape in current model is torch.Size([10, 640]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.batch_norm.num_batches_tracked", "module.fc.weight", "module.fc.bias". 
# C.2 'pre-act-18' | Different architecture
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=0, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='rand', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='pre-act-18', model_dataset='CIFAR10', model_id='pre-act-18', model_normalize=1, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_pre-act-18_model-normalized_data-unnormalized
File Directory: ./files/CIFAR10/model_pre-act-18_model-normalized_data-unnormalized
cuda:0
22-10-10 03:31
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.fc.weight", "module.fc.bias". 
	Unexpected key(s) in state_dict: "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
	size mismatch for module.conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
# --model_normalize 1, --data_normalize 1
# Teacher/Source/Victim model
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=1, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='rand', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='teacher', model_dataset='CIFAR10', model_id='teacher', model_normalize=1, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_teacher_model-normalized_data-normalized
File Directory: ./files/CIFAR10/model_teacher_model-normalized_data-normalized
cuda:0
22-10-10 03:31
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Model: ./models/CIFAR10/model_teacher_model-normalized_data-normalized | 	 Test Acc: 0.398
Getting random attacks
Number of steps = 50 | Failed to convert = 26 | Time taken = 5.884443283081055
Number of steps = 50 | Failed to convert = 21 | Time taken = 5.756078720092773
Number of steps = 50 | Failed to convert = 19 | Time taken = 5.681447267532349
Number of steps = 50 | Failed to convert = 22 | Time taken = 5.552016258239746
Number of steps = 50 | Failed to convert = 20 | Time taken = 5.9786810874938965
Number of steps = 50 | Failed to convert = 24 | Time taken = 5.758910655975342
Number of steps = 50 | Failed to convert = 24 | Time taken = 5.760526180267334
Number of steps = 50 | Failed to convert = 18 | Time taken = 5.575154781341553
Number of steps = 50 | Failed to convert = 18 | Time taken = 5.814502000808716
Number of steps = 50 | Failed to convert = 20 | Time taken = 5.643111228942871
Number of steps = 50 | Failed to convert = 7 | Time taken = 4.058410406112671
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.97003173828125
Number of steps = 50 | Failed to convert = 5 | Time taken = 4.051048040390015
Number of steps = 50 | Failed to convert = 8 | Time taken = 4.087515115737915
Number of steps = 50 | Failed to convert = 8 | Time taken = 4.104917526245117
Number of steps = 50 | Failed to convert = 10 | Time taken = 4.0138890743255615
Number of steps = 50 | Failed to convert = 8 | Time taken = 4.028111696243286
Number of steps = 50 | Failed to convert = 7 | Time taken = 4.04760479927063
Number of steps = 50 | Failed to convert = 6 | Time taken = 4.040392637252808
Number of steps = 50 | Failed to convert = 9 | Time taken = 4.12015438079834
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.281161308288574
Number of steps = 50 | Failed to convert = 7 | Time taken = 2.3188626766204834
Number of steps = 50 | Failed to convert = 6 | Time taken = 2.4100544452667236
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.367257833480835
Number of steps = 50 | Failed to convert = 7 | Time taken = 2.307105541229248
Number of steps = 50 | Failed to convert = 6 | Time taken = 2.3101425170898438
Number of steps = 50 | Failed to convert = 7 | Time taken = 2.343080997467041
Number of steps = 50 | Failed to convert = 9 | Time taken = 2.443842649459839
Number of steps = 50 | Failed to convert = 6 | Time taken = 2.2906503677368164
Number of steps = 50 | Failed to convert = 7 | Time taken = 2.4018120765686035
Number of steps = 50 | Failed to convert = 23 | Time taken = 5.257956266403198
Number of steps = 50 | Failed to convert = 19 | Time taken = 5.159196376800537
Number of steps = 50 | Failed to convert = 21 | Time taken = 5.079500436782837
Number of steps = 50 | Failed to convert = 22 | Time taken = 5.2204413414001465
Number of steps = 50 | Failed to convert = 23 | Time taken = 5.243320465087891
Number of steps = 50 | Failed to convert = 21 | Time taken = 5.258837699890137
Number of steps = 50 | Failed to convert = 23 | Time taken = 5.419609546661377
Number of steps = 50 | Failed to convert = 26 | Time taken = 5.251413822174072
Number of steps = 50 | Failed to convert = 28 | Time taken = 5.515044212341309
Number of steps = 50 | Failed to convert = 24 | Time taken = 5.1573827266693115
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.8034565448760986
Number of steps = 50 | Failed to convert = 8 | Time taken = 3.676487922668457
Number of steps = 50 | Failed to convert = 9 | Time taken = 3.780263900756836
Number of steps = 50 | Failed to convert = 7 | Time taken = 3.8633220195770264
Number of steps = 50 | Failed to convert = 9 | Time taken = 3.828117609024048
Number of steps = 50 | Failed to convert = 9 | Time taken = 3.7047646045684814
Number of steps = 50 | Failed to convert = 8 | Time taken = 3.856657028198242
Number of steps = 50 | Failed to convert = 7 | Time taken = 3.8313395977020264
Number of steps = 50 | Failed to convert = 9 | Time taken = 3.8846688270568848
Number of steps = 50 | Failed to convert = 9 | Time taken = 3.7388339042663574
Number of steps = 50 | Failed to convert = 7 | Time taken = 2.258347988128662
Number of steps = 50 | Failed to convert = 3 | Time taken = 2.0736846923828125
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.147578716278076
Number of steps = 50 | Failed to convert = 6 | Time taken = 2.2206976413726807
Number of steps = 50 | Failed to convert = 6 | Time taken = 2.2894861698150635
Number of steps = 50 | Failed to convert = 8 | Time taken = 2.256680965423584
Number of steps = 50 | Failed to convert = 8 | Time taken = 2.3582985401153564
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.122197389602661
Number of steps = 50 | Failed to convert = 6 | Time taken = 2.1489768028259277
Number of steps = 50 | Failed to convert = 6 | Time taken = 2.1866884231567383
torch.Size([1000, 10, 3])
tensor([[[1.2995e-01, 4.4221e+00, 1.5691e+02],
         [9.9970e-02, 5.6602e+00, 1.5140e+02],
         [1.7999e-01, 3.7927e+00, 1.5684e+02],
         ...,
         [1.5498e-01, 5.5689e+00, 1.5157e+02],
         [1.2998e-01, 4.1270e+00, 1.5651e+02],
         [1.4999e-01, 4.1032e+00, 1.8231e+02]],

        [[1.1500e-01, 3.8975e+00, 1.6891e+02],
         [1.1500e-01, 3.8256e+00, 1.6875e+02],
         [1.4495e-01, 3.9261e+00, 1.4536e+02],
         ...,
         [1.0992e-01, 3.3912e+00, 1.4571e+02],
         [1.4479e-01, 3.6157e+00, 1.7188e+02],
         [1.1997e-01, 3.6907e+00, 1.4163e+02]],

        [[6.9998e-02, 3.0413e+00, 1.5080e+02],
         [8.9935e-02, 2.5042e+00, 1.2033e+02],
         [8.4963e-02, 3.7109e+00, 1.4608e+02],
         ...,
         [1.0500e-01, 2.4985e+00, 1.5011e+02],
         [6.9988e-02, 3.8124e+00, 1.4991e+02],
         [9.9966e-02, 3.0136e+00, 1.2215e+02]],

        ...,

        [[4.9989e-03, 2.7578e-01, 3.0987e+01],
         [4.9982e-03, 2.7255e-01, 2.9978e+01],
         [4.9918e-03, 2.7714e-01, 3.0589e+01],
         ...,
         [4.9991e-03, 2.7513e-01, 3.0466e+01],
         [4.9993e-03, 2.7831e-01, 3.1350e+01],
         [4.9996e-03, 2.7679e-01, 3.0644e+01]],

        [[1.5998e-01, 4.1103e+00, 3.2309e+02],
         [1.3996e-01, 4.8394e+00, 2.0273e+02],
         [2.5500e-01, 5.0172e+00, 1.7904e+02],
         ...,
         [1.7476e-01, 1.2405e+01, 2.6309e+02],
         [1.7980e-01, 9.8151e+00, 2.0671e+02],
         [1.7987e-01, 5.6600e+00, 3.6355e+02]],

        [[8.4965e-02, 2.7396e+00, 9.4097e+01],
         [8.4983e-02, 2.4714e+00, 1.2391e+02],
         [7.4993e-02, 2.5128e+00, 9.2757e+01],
         ...,
         [7.4961e-02, 3.0312e+00, 1.2074e+02],
         [7.9987e-02, 2.7554e+00, 1.2273e+02],
         [7.9997e-02, 2.5684e+00, 1.2326e+02]]])
Getting random attacks
Number of steps = 50 | Failed to convert = 23 | Time taken = 4.899879693984985
Number of steps = 50 | Failed to convert = 22 | Time taken = 5.117111921310425
Number of steps = 50 | Failed to convert = 22 | Time taken = 5.07342529296875
Number of steps = 50 | Failed to convert = 21 | Time taken = 5.051308870315552
Number of steps = 50 | Failed to convert = 26 | Time taken = 5.316990375518799
Number of steps = 50 | Failed to convert = 21 | Time taken = 4.91223406791687
Number of steps = 50 | Failed to convert = 22 | Time taken = 4.8919219970703125
Number of steps = 50 | Failed to convert = 26 | Time taken = 4.995224714279175
Number of steps = 50 | Failed to convert = 24 | Time taken = 5.095837116241455
Number of steps = 50 | Failed to convert = 28 | Time taken = 5.053085803985596
Number of steps = 50 | Failed to convert = 8 | Time taken = 3.7530598640441895
Number of steps = 50 | Failed to convert = 5 | Time taken = 3.6083285808563232
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.6012558937072754
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.697312593460083
Number of steps = 50 | Failed to convert = 5 | Time taken = 3.5959789752960205
Number of steps = 50 | Failed to convert = 5 | Time taken = 3.690431594848633
Number of steps = 50 | Failed to convert = 5 | Time taken = 3.6764938831329346
Number of steps = 50 | Failed to convert = 5 | Time taken = 3.5814216136932373
Number of steps = 50 | Failed to convert = 7 | Time taken = 3.6885790824890137
Number of steps = 50 | Failed to convert = 7 | Time taken = 3.831437349319458
Number of steps = 50 | Failed to convert = 4 | Time taken = 2.1828389167785645
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.2870101928710938
Number of steps = 50 | Failed to convert = 6 | Time taken = 2.206458568572998
Number of steps = 50 | Failed to convert = 4 | Time taken = 2.0391416549682617
Number of steps = 50 | Failed to convert = 3 | Time taken = 2.0714871883392334
Number of steps = 50 | Failed to convert = 3 | Time taken = 2.0791263580322266
Number of steps = 50 | Failed to convert = 6 | Time taken = 2.2058537006378174
Number of steps = 50 | Failed to convert = 2 | Time taken = 2.050133466720581
Number of steps = 50 | Failed to convert = 4 | Time taken = 2.077693462371826
Number of steps = 50 | Failed to convert = 4 | Time taken = 2.0737221240997314
Number of steps = 50 | Failed to convert = 24 | Time taken = 5.9223809242248535
Number of steps = 50 | Failed to convert = 23 | Time taken = 5.383356809616089
Number of steps = 50 | Failed to convert = 26 | Time taken = 5.618899345397949
Number of steps = 50 | Failed to convert = 27 | Time taken = 5.806373119354248
Number of steps = 50 | Failed to convert = 21 | Time taken = 5.519690036773682
Number of steps = 50 | Failed to convert = 31 | Time taken = 5.704540014266968
Number of steps = 50 | Failed to convert = 29 | Time taken = 5.660046339035034
Number of steps = 50 | Failed to convert = 22 | Time taken = 5.5634119510650635
Number of steps = 50 | Failed to convert = 24 | Time taken = 5.775635242462158
Number of steps = 50 | Failed to convert = 26 | Time taken = 5.61391019821167
Number of steps = 50 | Failed to convert = 7 | Time taken = 4.176702976226807
Number of steps = 50 | Failed to convert = 7 | Time taken = 3.9887218475341797
Number of steps = 50 | Failed to convert = 7 | Time taken = 3.8926782608032227
Number of steps = 50 | Failed to convert = 6 | Time taken = 4.048342943191528
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.963365077972412
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.8758704662323
Number of steps = 50 | Failed to convert = 6 | Time taken = 3.961171865463257
Number of steps = 50 | Failed to convert = 8 | Time taken = 4.274474620819092
Number of steps = 50 | Failed to convert = 8 | Time taken = 4.059805154800415
Number of steps = 50 | Failed to convert = 8 | Time taken = 4.018604040145874
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.328856945037842
Number of steps = 50 | Failed to convert = 7 | Time taken = 2.346766710281372
Number of steps = 50 | Failed to convert = 3 | Time taken = 2.1858794689178467
Number of steps = 50 | Failed to convert = 4 | Time taken = 2.2093913555145264
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.320718765258789
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.3210926055908203
Number of steps = 50 | Failed to convert = 4 | Time taken = 2.131181240081787
Number of steps = 50 | Failed to convert = 4 | Time taken = 2.195594549179077
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.3335068225860596
Number of steps = 50 | Failed to convert = 5 | Time taken = 2.2888951301574707
torch.Size([1000, 10, 3])
tensor([[[4.9866e-03, 2.8191e-01, 3.0276e+01],
         [4.9986e-03, 2.7315e-01, 3.0147e+01],
         [4.9995e-03, 2.7907e-01, 3.0648e+01],
         ...,
         [4.9983e-03, 2.7236e-01, 3.0956e+01],
         [4.9990e-03, 2.6929e-01, 3.0268e+01],
         [4.9938e-03, 2.8034e-01, 3.1367e+01]],

        [[1.1496e-01, 3.5409e+00, 1.5672e+02],
         [1.1500e-01, 3.6777e+00, 1.5738e+02],
         [1.1986e-01, 4.3324e+00, 1.5413e+02],
         ...,
         [9.9973e-02, 4.3942e+00, 1.5570e+02],
         [1.1999e-01, 4.9656e+00, 1.5790e+02],
         [1.1497e-01, 4.5848e+00, 1.5516e+02]],

        [[4.9985e-03, 2.7642e-01, 2.9684e+01],
         [4.9962e-03, 2.7441e-01, 3.0280e+01],
         [4.9977e-03, 2.7508e-01, 3.0148e+01],
         ...,
         [4.9933e-03, 2.7140e-01, 3.0220e+01],
         [4.9997e-03, 2.6900e-01, 2.9846e+01],
         [4.9960e-03, 2.7856e-01, 2.9413e+01]],

        ...,

        [[4.9975e-03, 2.7478e-01, 3.0407e+01],
         [4.9989e-03, 2.7998e-01, 2.9969e+01],
         [4.9964e-03, 2.7583e-01, 3.0307e+01],
         ...,
         [4.9983e-03, 2.8132e-01, 1.8014e+02],
         [4.9981e-03, 2.7439e-01, 3.1150e+01],
         [4.9996e-03, 2.7862e-01, 1.5069e+02]],

        [[4.9956e-03, 2.7845e-01, 3.0876e+01],
         [4.9993e-03, 2.8149e-01, 3.1792e+01],
         [4.9986e-03, 2.7513e-01, 3.0323e+01],
         ...,
         [4.9964e-03, 2.8299e-01, 3.0453e+01],
         [4.9999e-03, 2.7600e-01, 3.0378e+01],
         [4.9959e-03, 2.8086e-01, 3.0628e+01]],

        [[6.9954e-02, 1.3662e+00, 2.9984e+01],
         [2.4993e-02, 1.3834e+00, 3.0006e+01],
         [3.4968e-02, 5.5729e-01, 6.1698e+01],
         ...,
         [4.4997e-02, 1.3769e+00, 8.9024e+01],
         [3.9998e-02, 1.0959e+00, 6.1508e+01],
         [3.9999e-02, 1.3887e+00, 5.9231e+01]]])
Time taken: 506.97 s
# 'independent'
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=1, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='rand', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='independent', model_dataset='CIFAR10', model_id='independent', model_normalize=1, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_independent_model-normalized_data-normalized
File Directory: ./files/CIFAR10/model_independent_model-normalized_data-normalized
cuda:0
22-10-10 03:40
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight". 
	size mismatch for module.block1.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 16, 3, 3]).
	size mismatch for module.block1.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block1.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block1.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([160, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([160]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 160, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([320, 160, 1, 1]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block2.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block2.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 320, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.0.conv_inp.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([640, 320, 1, 1]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.batch_norm2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.block3.nblockLayer.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.block3.nblockLayer.1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([640, 640, 3, 3]).
	size mismatch for module.batch_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.batch_norm.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([640]).
	size mismatch for module.fc.weight: copying a param with shape torch.Size([10, 64]) from checkpoint, the shape in current model is torch.Size([10, 640]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm1.num_batches_tracked", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.batch_norm2.num_batches_tracked", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.batch_norm.num_batches_tracked", "module.fc.weight", "module.fc.bias". 
# C.2 'pre-act-18' | Different architecture
Namespace(alpha_l_1=1.0, alpha_l_2=0.01, alpha_l_inf=0.001, batch_size=500, combine_ratio=0.5, concat=0, concat_factor=1.0, config_file=None, data_normalize=1, data_path=None, dataset='CIFAR10', device='cuda:0', distance=None, dropRate=0.0, epochs=50, epsilon_l_1=12, epsilon_l_2=0.5, epsilon_l_inf=0.03137254901960784, experiment='normalization', feature_type='rand', gap=0.001, gpu_id=0, imagenet_architecture='wrn', k=100, lr_max=0.1, lr_min=0.0, lr_mode=1, mode='pre-act-18', model_dataset='CIFAR10', model_id='pre-act-18', model_normalize=1, model_type='wrn-28-10', noise_sigma=0.05, normalize=1, num_iter=500, opt_type='SGD', path=None, pseudo_labels=0, randomize=0, regressor_embed=0, restarts=1, resume=0, resume_iter=-1, reverse_train_test=0, seed=0, smallest_adv=1, target_batch_size=0, target_epoch=0, target_te_acc=0, target_tr_acc=0)
Model Directory: ./models/CIFAR10/model_pre-act-18_model-normalized_data-normalized
File Directory: ./files/CIFAR10/model_pre-act-18_model-normalized_data-normalized
cuda:0
22-10-10 03:40
No Transform
Files already downloaded and verified
Files already downloaded and verified
No Transform
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "./src/generate_features.py", line 197, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device)) 
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.block1.nblockLayer.0.batch_norm1.weight", "module.block1.nblockLayer.0.batch_norm1.bias", "module.block1.nblockLayer.0.batch_norm1.running_mean", "module.block1.nblockLayer.0.batch_norm1.running_var", "module.block1.nblockLayer.0.batch_norm2.weight", "module.block1.nblockLayer.0.batch_norm2.bias", "module.block1.nblockLayer.0.batch_norm2.running_mean", "module.block1.nblockLayer.0.batch_norm2.running_var", "module.block1.nblockLayer.0.conv1.weight", "module.block1.nblockLayer.0.conv2.weight", "module.block1.nblockLayer.0.conv_inp.weight", "module.block1.nblockLayer.1.batch_norm1.weight", "module.block1.nblockLayer.1.batch_norm1.bias", "module.block1.nblockLayer.1.batch_norm1.running_mean", "module.block1.nblockLayer.1.batch_norm1.running_var", "module.block1.nblockLayer.1.batch_norm2.weight", "module.block1.nblockLayer.1.batch_norm2.bias", "module.block1.nblockLayer.1.batch_norm2.running_mean", "module.block1.nblockLayer.1.batch_norm2.running_var", "module.block1.nblockLayer.1.conv1.weight", "module.block1.nblockLayer.1.conv2.weight", "module.block1.nblockLayer.2.batch_norm1.weight", "module.block1.nblockLayer.2.batch_norm1.bias", "module.block1.nblockLayer.2.batch_norm1.running_mean", "module.block1.nblockLayer.2.batch_norm1.running_var", "module.block1.nblockLayer.2.batch_norm2.weight", "module.block1.nblockLayer.2.batch_norm2.bias", "module.block1.nblockLayer.2.batch_norm2.running_mean", "module.block1.nblockLayer.2.batch_norm2.running_var", "module.block1.nblockLayer.2.conv1.weight", "module.block1.nblockLayer.2.conv2.weight", "module.block1.nblockLayer.3.batch_norm1.weight", "module.block1.nblockLayer.3.batch_norm1.bias", "module.block1.nblockLayer.3.batch_norm1.running_mean", "module.block1.nblockLayer.3.batch_norm1.running_var", "module.block1.nblockLayer.3.batch_norm2.weight", "module.block1.nblockLayer.3.batch_norm2.bias", "module.block1.nblockLayer.3.batch_norm2.running_mean", "module.block1.nblockLayer.3.batch_norm2.running_var", "module.block1.nblockLayer.3.conv1.weight", "module.block1.nblockLayer.3.conv2.weight", "module.block2.nblockLayer.0.batch_norm1.weight", "module.block2.nblockLayer.0.batch_norm1.bias", "module.block2.nblockLayer.0.batch_norm1.running_mean", "module.block2.nblockLayer.0.batch_norm1.running_var", "module.block2.nblockLayer.0.batch_norm2.weight", "module.block2.nblockLayer.0.batch_norm2.bias", "module.block2.nblockLayer.0.batch_norm2.running_mean", "module.block2.nblockLayer.0.batch_norm2.running_var", "module.block2.nblockLayer.0.conv1.weight", "module.block2.nblockLayer.0.conv2.weight", "module.block2.nblockLayer.0.conv_inp.weight", "module.block2.nblockLayer.1.batch_norm1.weight", "module.block2.nblockLayer.1.batch_norm1.bias", "module.block2.nblockLayer.1.batch_norm1.running_mean", "module.block2.nblockLayer.1.batch_norm1.running_var", "module.block2.nblockLayer.1.batch_norm2.weight", "module.block2.nblockLayer.1.batch_norm2.bias", "module.block2.nblockLayer.1.batch_norm2.running_mean", "module.block2.nblockLayer.1.batch_norm2.running_var", "module.block2.nblockLayer.1.conv1.weight", "module.block2.nblockLayer.1.conv2.weight", "module.block2.nblockLayer.2.batch_norm1.weight", "module.block2.nblockLayer.2.batch_norm1.bias", "module.block2.nblockLayer.2.batch_norm1.running_mean", "module.block2.nblockLayer.2.batch_norm1.running_var", "module.block2.nblockLayer.2.batch_norm2.weight", "module.block2.nblockLayer.2.batch_norm2.bias", "module.block2.nblockLayer.2.batch_norm2.running_mean", "module.block2.nblockLayer.2.batch_norm2.running_var", "module.block2.nblockLayer.2.conv1.weight", "module.block2.nblockLayer.2.conv2.weight", "module.block2.nblockLayer.3.batch_norm1.weight", "module.block2.nblockLayer.3.batch_norm1.bias", "module.block2.nblockLayer.3.batch_norm1.running_mean", "module.block2.nblockLayer.3.batch_norm1.running_var", "module.block2.nblockLayer.3.batch_norm2.weight", "module.block2.nblockLayer.3.batch_norm2.bias", "module.block2.nblockLayer.3.batch_norm2.running_mean", "module.block2.nblockLayer.3.batch_norm2.running_var", "module.block2.nblockLayer.3.conv1.weight", "module.block2.nblockLayer.3.conv2.weight", "module.block3.nblockLayer.0.batch_norm1.weight", "module.block3.nblockLayer.0.batch_norm1.bias", "module.block3.nblockLayer.0.batch_norm1.running_mean", "module.block3.nblockLayer.0.batch_norm1.running_var", "module.block3.nblockLayer.0.batch_norm2.weight", "module.block3.nblockLayer.0.batch_norm2.bias", "module.block3.nblockLayer.0.batch_norm2.running_mean", "module.block3.nblockLayer.0.batch_norm2.running_var", "module.block3.nblockLayer.0.conv1.weight", "module.block3.nblockLayer.0.conv2.weight", "module.block3.nblockLayer.0.conv_inp.weight", "module.block3.nblockLayer.1.batch_norm1.weight", "module.block3.nblockLayer.1.batch_norm1.bias", "module.block3.nblockLayer.1.batch_norm1.running_mean", "module.block3.nblockLayer.1.batch_norm1.running_var", "module.block3.nblockLayer.1.batch_norm2.weight", "module.block3.nblockLayer.1.batch_norm2.bias", "module.block3.nblockLayer.1.batch_norm2.running_mean", "module.block3.nblockLayer.1.batch_norm2.running_var", "module.block3.nblockLayer.1.conv1.weight", "module.block3.nblockLayer.1.conv2.weight", "module.block3.nblockLayer.2.batch_norm1.weight", "module.block3.nblockLayer.2.batch_norm1.bias", "module.block3.nblockLayer.2.batch_norm1.running_mean", "module.block3.nblockLayer.2.batch_norm1.running_var", "module.block3.nblockLayer.2.batch_norm2.weight", "module.block3.nblockLayer.2.batch_norm2.bias", "module.block3.nblockLayer.2.batch_norm2.running_mean", "module.block3.nblockLayer.2.batch_norm2.running_var", "module.block3.nblockLayer.2.conv1.weight", "module.block3.nblockLayer.2.conv2.weight", "module.block3.nblockLayer.3.batch_norm1.weight", "module.block3.nblockLayer.3.batch_norm1.bias", "module.block3.nblockLayer.3.batch_norm1.running_mean", "module.block3.nblockLayer.3.batch_norm1.running_var", "module.block3.nblockLayer.3.batch_norm2.weight", "module.block3.nblockLayer.3.batch_norm2.bias", "module.block3.nblockLayer.3.batch_norm2.running_mean", "module.block3.nblockLayer.3.batch_norm2.running_var", "module.block3.nblockLayer.3.conv1.weight", "module.block3.nblockLayer.3.conv2.weight", "module.batch_norm.weight", "module.batch_norm.bias", "module.batch_norm.running_mean", "module.batch_norm.running_var", "module.fc.weight", "module.fc.bias". 
	Unexpected key(s) in state_dict: "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
	size mismatch for module.conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./src/generate_features.py", line 427, in <module>
    feature_extractor(args)
  File "./src/generate_features.py", line 200, in feature_extractor
    student.load_state_dict(torch.load(location, map_location = args.device))   
  File "/home/user/tingweijing/env/DIenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.module.conv1.weight", "module.module.block1.nblockLayer.0.batch_norm1.weight", "module.module.block1.nblockLayer.0.batch_norm1.bias", "module.module.block1.nblockLayer.0.batch_norm1.running_mean", "module.module.block1.nblockLayer.0.batch_norm1.running_var", "module.module.block1.nblockLayer.0.batch_norm2.weight", "module.module.block1.nblockLayer.0.batch_norm2.bias", "module.module.block1.nblockLayer.0.batch_norm2.running_mean", "module.module.block1.nblockLayer.0.batch_norm2.running_var", "module.module.block1.nblockLayer.0.conv1.weight", "module.module.block1.nblockLayer.0.conv2.weight", "module.module.block1.nblockLayer.0.conv_inp.weight", "module.module.block1.nblockLayer.1.batch_norm1.weight", "module.module.block1.nblockLayer.1.batch_norm1.bias", "module.module.block1.nblockLayer.1.batch_norm1.running_mean", "module.module.block1.nblockLayer.1.batch_norm1.running_var", "module.module.block1.nblockLayer.1.batch_norm2.weight", "module.module.block1.nblockLayer.1.batch_norm2.bias", "module.module.block1.nblockLayer.1.batch_norm2.running_mean", "module.module.block1.nblockLayer.1.batch_norm2.running_var", "module.module.block1.nblockLayer.1.conv1.weight", "module.module.block1.nblockLayer.1.conv2.weight", "module.module.block1.nblockLayer.2.batch_norm1.weight", "module.module.block1.nblockLayer.2.batch_norm1.bias", "module.module.block1.nblockLayer.2.batch_norm1.running_mean", "module.module.block1.nblockLayer.2.batch_norm1.running_var", "module.module.block1.nblockLayer.2.batch_norm2.weight", "module.module.block1.nblockLayer.2.batch_norm2.bias", "module.module.block1.nblockLayer.2.batch_norm2.running_mean", "module.module.block1.nblockLayer.2.batch_norm2.running_var", "module.module.block1.nblockLayer.2.conv1.weight", "module.module.block1.nblockLayer.2.conv2.weight", "module.module.block1.nblockLayer.3.batch_norm1.weight", "module.module.block1.nblockLayer.3.batch_norm1.bias", "module.module.block1.nblockLayer.3.batch_norm1.running_mean", "module.module.block1.nblockLayer.3.batch_norm1.running_var", "module.module.block1.nblockLayer.3.batch_norm2.weight", "module.module.block1.nblockLayer.3.batch_norm2.bias", "module.module.block1.nblockLayer.3.batch_norm2.running_mean", "module.module.block1.nblockLayer.3.batch_norm2.running_var", "module.module.block1.nblockLayer.3.conv1.weight", "module.module.block1.nblockLayer.3.conv2.weight", "module.module.block2.nblockLayer.0.batch_norm1.weight", "module.module.block2.nblockLayer.0.batch_norm1.bias", "module.module.block2.nblockLayer.0.batch_norm1.running_mean", "module.module.block2.nblockLayer.0.batch_norm1.running_var", "module.module.block2.nblockLayer.0.batch_norm2.weight", "module.module.block2.nblockLayer.0.batch_norm2.bias", "module.module.block2.nblockLayer.0.batch_norm2.running_mean", "module.module.block2.nblockLayer.0.batch_norm2.running_var", "module.module.block2.nblockLayer.0.conv1.weight", "module.module.block2.nblockLayer.0.conv2.weight", "module.module.block2.nblockLayer.0.conv_inp.weight", "module.module.block2.nblockLayer.1.batch_norm1.weight", "module.module.block2.nblockLayer.1.batch_norm1.bias", "module.module.block2.nblockLayer.1.batch_norm1.running_mean", "module.module.block2.nblockLayer.1.batch_norm1.running_var", "module.module.block2.nblockLayer.1.batch_norm2.weight", "module.module.block2.nblockLayer.1.batch_norm2.bias", "module.module.block2.nblockLayer.1.batch_norm2.running_mean", "module.module.block2.nblockLayer.1.batch_norm2.running_var", "module.module.block2.nblockLayer.1.conv1.weight", "module.module.block2.nblockLayer.1.conv2.weight", "module.module.block2.nblockLayer.2.batch_norm1.weight", "module.module.block2.nblockLayer.2.batch_norm1.bias", "module.module.block2.nblockLayer.2.batch_norm1.running_mean", "module.module.block2.nblockLayer.2.batch_norm1.running_var", "module.module.block2.nblockLayer.2.batch_norm2.weight", "module.module.block2.nblockLayer.2.batch_norm2.bias", "module.module.block2.nblockLayer.2.batch_norm2.running_mean", "module.module.block2.nblockLayer.2.batch_norm2.running_var", "module.module.block2.nblockLayer.2.conv1.weight", "module.module.block2.nblockLayer.2.conv2.weight", "module.module.block2.nblockLayer.3.batch_norm1.weight", "module.module.block2.nblockLayer.3.batch_norm1.bias", "module.module.block2.nblockLayer.3.batch_norm1.running_mean", "module.module.block2.nblockLayer.3.batch_norm1.running_var", "module.module.block2.nblockLayer.3.batch_norm2.weight", "module.module.block2.nblockLayer.3.batch_norm2.bias", "module.module.block2.nblockLayer.3.batch_norm2.running_mean", "module.module.block2.nblockLayer.3.batch_norm2.running_var", "module.module.block2.nblockLayer.3.conv1.weight", "module.module.block2.nblockLayer.3.conv2.weight", "module.module.block3.nblockLayer.0.batch_norm1.weight", "module.module.block3.nblockLayer.0.batch_norm1.bias", "module.module.block3.nblockLayer.0.batch_norm1.running_mean", "module.module.block3.nblockLayer.0.batch_norm1.running_var", "module.module.block3.nblockLayer.0.batch_norm2.weight", "module.module.block3.nblockLayer.0.batch_norm2.bias", "module.module.block3.nblockLayer.0.batch_norm2.running_mean", "module.module.block3.nblockLayer.0.batch_norm2.running_var", "module.module.block3.nblockLayer.0.conv1.weight", "module.module.block3.nblockLayer.0.conv2.weight", "module.module.block3.nblockLayer.0.conv_inp.weight", "module.module.block3.nblockLayer.1.batch_norm1.weight", "module.module.block3.nblockLayer.1.batch_norm1.bias", "module.module.block3.nblockLayer.1.batch_norm1.running_mean", "module.module.block3.nblockLayer.1.batch_norm1.running_var", "module.module.block3.nblockLayer.1.batch_norm2.weight", "module.module.block3.nblockLayer.1.batch_norm2.bias", "module.module.block3.nblockLayer.1.batch_norm2.running_mean", "module.module.block3.nblockLayer.1.batch_norm2.running_var", "module.module.block3.nblockLayer.1.conv1.weight", "module.module.block3.nblockLayer.1.conv2.weight", "module.module.block3.nblockLayer.2.batch_norm1.weight", "module.module.block3.nblockLayer.2.batch_norm1.bias", "module.module.block3.nblockLayer.2.batch_norm1.running_mean", "module.module.block3.nblockLayer.2.batch_norm1.running_var", "module.module.block3.nblockLayer.2.batch_norm2.weight", "module.module.block3.nblockLayer.2.batch_norm2.bias", "module.module.block3.nblockLayer.2.batch_norm2.running_mean", "module.module.block3.nblockLayer.2.batch_norm2.running_var", "module.module.block3.nblockLayer.2.conv1.weight", "module.module.block3.nblockLayer.2.conv2.weight", "module.module.block3.nblockLayer.3.batch_norm1.weight", "module.module.block3.nblockLayer.3.batch_norm1.bias", "module.module.block3.nblockLayer.3.batch_norm1.running_mean", "module.module.block3.nblockLayer.3.batch_norm1.running_var", "module.module.block3.nblockLayer.3.batch_norm2.weight", "module.module.block3.nblockLayer.3.batch_norm2.bias", "module.module.block3.nblockLayer.3.batch_norm2.running_mean", "module.module.block3.nblockLayer.3.batch_norm2.running_var", "module.module.block3.nblockLayer.3.conv1.weight", "module.module.block3.nblockLayer.3.conv2.weight", "module.module.batch_norm.weight", "module.module.batch_norm.bias", "module.module.batch_norm.running_mean", "module.module.batch_norm.running_var", "module.module.fc.weight", "module.module.fc.bias". 
	Unexpected key(s) in state_dict: "module.conv1.weight", "module.layer1.0.bn1.weight", "module.layer1.0.bn1.bias", "module.layer1.0.bn1.running_mean", "module.layer1.0.bn1.running_var", "module.layer1.0.bn1.num_batches_tracked", "module.layer1.0.conv1.weight", "module.layer1.0.bn2.weight", "module.layer1.0.bn2.bias", "module.layer1.0.bn2.running_mean", "module.layer1.0.bn2.running_var", "module.layer1.0.bn2.num_batches_tracked", "module.layer1.0.conv2.weight", "module.layer1.1.bn1.weight", "module.layer1.1.bn1.bias", "module.layer1.1.bn1.running_mean", "module.layer1.1.bn1.running_var", "module.layer1.1.bn1.num_batches_tracked", "module.layer1.1.conv1.weight", "module.layer1.1.bn2.weight", "module.layer1.1.bn2.bias", "module.layer1.1.bn2.running_mean", "module.layer1.1.bn2.running_var", "module.layer1.1.bn2.num_batches_tracked", "module.layer1.1.conv2.weight", "module.layer2.0.bn1.weight", "module.layer2.0.bn1.bias", "module.layer2.0.bn1.running_mean", "module.layer2.0.bn1.running_var", "module.layer2.0.bn1.num_batches_tracked", "module.layer2.0.conv1.weight", "module.layer2.0.bn2.weight", "module.layer2.0.bn2.bias", "module.layer2.0.bn2.running_mean", "module.layer2.0.bn2.running_var", "module.layer2.0.bn2.num_batches_tracked", "module.layer2.0.conv2.weight", "module.layer2.0.shortcut.0.weight", "module.layer2.1.bn1.weight", "module.layer2.1.bn1.bias", "module.layer2.1.bn1.running_mean", "module.layer2.1.bn1.running_var", "module.layer2.1.bn1.num_batches_tracked", "module.layer2.1.conv1.weight", "module.layer2.1.bn2.weight", "module.layer2.1.bn2.bias", "module.layer2.1.bn2.running_mean", "module.layer2.1.bn2.running_var", "module.layer2.1.bn2.num_batches_tracked", "module.layer2.1.conv2.weight", "module.layer3.0.bn1.weight", "module.layer3.0.bn1.bias", "module.layer3.0.bn1.running_mean", "module.layer3.0.bn1.running_var", "module.layer3.0.bn1.num_batches_tracked", "module.layer3.0.conv1.weight", "module.layer3.0.bn2.weight", "module.layer3.0.bn2.bias", "module.layer3.0.bn2.running_mean", "module.layer3.0.bn2.running_var", "module.layer3.0.bn2.num_batches_tracked", "module.layer3.0.conv2.weight", "module.layer3.0.shortcut.0.weight", "module.layer3.1.bn1.weight", "module.layer3.1.bn1.bias", "module.layer3.1.bn1.running_mean", "module.layer3.1.bn1.running_var", "module.layer3.1.bn1.num_batches_tracked", "module.layer3.1.conv1.weight", "module.layer3.1.bn2.weight", "module.layer3.1.bn2.bias", "module.layer3.1.bn2.running_mean", "module.layer3.1.bn2.running_var", "module.layer3.1.bn2.num_batches_tracked", "module.layer3.1.conv2.weight", "module.layer4.0.bn1.weight", "module.layer4.0.bn1.bias", "module.layer4.0.bn1.running_mean", "module.layer4.0.bn1.running_var", "module.layer4.0.bn1.num_batches_tracked", "module.layer4.0.conv1.weight", "module.layer4.0.bn2.weight", "module.layer4.0.bn2.bias", "module.layer4.0.bn2.running_mean", "module.layer4.0.bn2.running_var", "module.layer4.0.bn2.num_batches_tracked", "module.layer4.0.conv2.weight", "module.layer4.0.shortcut.0.weight", "module.layer4.1.bn1.weight", "module.layer4.1.bn1.bias", "module.layer4.1.bn1.running_mean", "module.layer4.1.bn1.running_var", "module.layer4.1.bn1.num_batches_tracked", "module.layer4.1.conv1.weight", "module.layer4.1.bn2.weight", "module.layer4.1.bn2.bias", "module.layer4.1.bn2.running_mean", "module.layer4.1.bn2.running_var", "module.layer4.1.bn2.num_batches_tracked", "module.layer4.1.conv2.weight", "module.linear.weight", "module.linear.bias". 
